{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Module\n",
    "\n",
    "> Model performance evaluation and validation metrics.\n",
    "\n",
    "This module provides:\n",
    "- `validate_classifications()`: Evaluate model on test set\n",
    "- `cross_validate()`: k-fold cross-validation\n",
    "- `ClassificationMetrics`: Accuracy, precision, recall, F1, Cohen's kappa\n",
    "- `ValidationResult`: Complete validation results with confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from typing import Optional, List, Dict, Tuple, Any\n",
    "import json\n",
    "import logging\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    cohen_kappa_score,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from openness_classifier.core import (\n",
    "    OpennessCategory,\n",
    "    ClassificationType,\n",
    "    Classification,\n",
    "    LLMConfiguration,\n",
    ")\n",
    "from openness_classifier.config import ClassifierConfig, load_config\n",
    "from openness_classifier.data import (\n",
    "    TrainingExample,\n",
    "    load_training_data,\n",
    "    train_test_split,\n",
    "    EmbeddingModel,\n",
    "    compute_embeddings,\n",
    ")\n",
    "from openness_classifier.classifier import OpennessClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Data Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class ClassificationMetrics:\n",
    "    \"\"\"Classification performance metrics for a single type (data or code).\n",
    "    \n",
    "    Attributes:\n",
    "        accuracy: Overall accuracy\n",
    "        precision_per_class: Precision for each category\n",
    "        recall_per_class: Recall for each category\n",
    "        f1_per_class: F1 score for each category\n",
    "        macro_f1: Macro-averaged F1\n",
    "        weighted_f1: Weighted F1 by class support\n",
    "        cohens_kappa: Inter-rater agreement (vs human coders)\n",
    "        support_per_class: Number of samples per class\n",
    "    \"\"\"\n",
    "    accuracy: float\n",
    "    precision_per_class: Dict[str, float]\n",
    "    recall_per_class: Dict[str, float]\n",
    "    f1_per_class: Dict[str, float]\n",
    "    macro_f1: float\n",
    "    weighted_f1: float\n",
    "    cohens_kappa: float\n",
    "    support_per_class: Dict[str, int]\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert to dictionary.\"\"\"\n",
    "        return {\n",
    "            'accuracy': self.accuracy,\n",
    "            'precision_per_class': self.precision_per_class,\n",
    "            'recall_per_class': self.recall_per_class,\n",
    "            'f1_per_class': self.f1_per_class,\n",
    "            'macro_f1': self.macro_f1,\n",
    "            'weighted_f1': self.weighted_f1,\n",
    "            'cohens_kappa': self.cohens_kappa,\n",
    "            'support_per_class': self.support_per_class,\n",
    "        }\n",
    "    \n",
    "    def to_markdown(self) -> str:\n",
    "        \"\"\"Generate markdown table for manuscript.\"\"\"\n",
    "        lines = [\n",
    "            \"| Metric | Value |\",\n",
    "            \"|--------|-------|\",\n",
    "            f\"| Accuracy | {self.accuracy:.3f} |\",\n",
    "            f\"| Macro F1 | {self.macro_f1:.3f} |\",\n",
    "            f\"| Weighted F1 | {self.weighted_f1:.3f} |\",\n",
    "            f\"| Cohen's Kappa | {self.cohens_kappa:.3f} |\",\n",
    "            \"\",\n",
    "            \"| Class | Precision | Recall | F1 | Support |\",\n",
    "            \"|-------|-----------|--------|----|---------|\",\n",
    "        ]\n",
    "        \n",
    "        for cls in ['open', 'mostly_open', 'mostly_closed', 'closed']:\n",
    "            if cls in self.precision_per_class:\n",
    "                lines.append(\n",
    "                    f\"| {cls} | {self.precision_per_class[cls]:.3f} | \"\n",
    "                    f\"{self.recall_per_class[cls]:.3f} | \"\n",
    "                    f\"{self.f1_per_class[cls]:.3f} | \"\n",
    "                    f\"{self.support_per_class[cls]} |\"\n",
    "                )\n",
    "        \n",
    "        return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class ValidationResult:\n",
    "    \"\"\"Complete validation results with metrics and confusion matrices.\n",
    "    \n",
    "    Attributes:\n",
    "        data_metrics: Metrics for data classification\n",
    "        code_metrics: Metrics for code classification\n",
    "        overall_accuracy: Combined accuracy\n",
    "        test_set_size: Number of test samples\n",
    "        train_set_size: Number of training samples\n",
    "        validation_timestamp: When validation was performed\n",
    "        model_config: LLM configuration used\n",
    "        confusion_matrices: {\"data\": matrix, \"code\": matrix}\n",
    "        misclassified_examples: List of misclassified statements\n",
    "    \"\"\"\n",
    "    data_metrics: Optional[ClassificationMetrics] = None\n",
    "    code_metrics: Optional[ClassificationMetrics] = None\n",
    "    overall_accuracy: float = 0.0\n",
    "    test_set_size: int = 0\n",
    "    train_set_size: int = 0\n",
    "    validation_timestamp: datetime = field(default_factory=datetime.utcnow)\n",
    "    model_config: Optional[LLMConfiguration] = None\n",
    "    confusion_matrices: Dict[str, np.ndarray] = field(default_factory=dict)\n",
    "    misclassified_examples: List[Tuple[str, str, str]] = field(default_factory=list)\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n",
    "        return {\n",
    "            'data_metrics': self.data_metrics.to_dict() if self.data_metrics else None,\n",
    "            'code_metrics': self.code_metrics.to_dict() if self.code_metrics else None,\n",
    "            'overall_accuracy': self.overall_accuracy,\n",
    "            'test_set_size': self.test_set_size,\n",
    "            'train_set_size': self.train_set_size,\n",
    "            'validation_timestamp': self.validation_timestamp.isoformat(),\n",
    "            'model_config': self.model_config.to_dict() if self.model_config else None,\n",
    "            'confusion_matrices': {\n",
    "                k: v.tolist() for k, v in self.confusion_matrices.items()\n",
    "            },\n",
    "            'misclassified_count': len(self.misclassified_examples),\n",
    "        }\n",
    "    \n",
    "    def to_json(self, path: Optional[str | Path] = None) -> str:\n",
    "        \"\"\"Export to JSON.\"\"\"\n",
    "        json_str = json.dumps(self.to_dict(), indent=2)\n",
    "        if path:\n",
    "            Path(path).write_text(json_str)\n",
    "        return json_str\n",
    "    \n",
    "    def to_markdown(self) -> str:\n",
    "        \"\"\"Generate markdown report for manuscript.\"\"\"\n",
    "        lines = [\n",
    "            \"# Validation Results\",\n",
    "            \"\",\n",
    "            f\"**Date**: {self.validation_timestamp.strftime('%Y-%m-%d %H:%M UTC')}\",\n",
    "            f\"**Test Set Size**: {self.test_set_size}\",\n",
    "            f\"**Training Set Size**: {self.train_set_size}\",\n",
    "            f\"**Overall Accuracy**: {self.overall_accuracy:.1%}\",\n",
    "            \"\",\n",
    "        ]\n",
    "        \n",
    "        if self.data_metrics:\n",
    "            lines.extend([\n",
    "                \"## Data Classification Metrics\",\n",
    "                \"\",\n",
    "                self.data_metrics.to_markdown(),\n",
    "                \"\",\n",
    "            ])\n",
    "        \n",
    "        if self.code_metrics:\n",
    "            lines.extend([\n",
    "                \"## Code Classification Metrics\",\n",
    "                \"\",\n",
    "                self.code_metrics.to_markdown(),\n",
    "                \"\",\n",
    "            ])\n",
    "        \n",
    "        return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_metrics(\n",
    "    y_true: List[str],\n",
    "    y_pred: List[str],\n",
    "    labels: Optional[List[str]] = None\n",
    ") -> ClassificationMetrics:\n",
    "    \"\"\"Compute classification metrics from true and predicted labels.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth labels\n",
    "        y_pred: Predicted labels\n",
    "        labels: Label names (default: openness categories)\n",
    "        \n",
    "    Returns:\n",
    "        ClassificationMetrics with all computed values\n",
    "    \"\"\"\n",
    "    if labels is None:\n",
    "        labels = ['open', 'mostly_open', 'mostly_closed', 'closed']\n",
    "    \n",
    "    # Filter to only labels that appear in data\n",
    "    present_labels = sorted(set(y_true) | set(y_pred))\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=present_labels, zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Macro and weighted F1\n",
    "    _, _, macro_f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='macro', zero_division=0\n",
    "    )\n",
    "    _, _, weighted_f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Cohen's kappa\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    \n",
    "    return ClassificationMetrics(\n",
    "        accuracy=accuracy,\n",
    "        precision_per_class=dict(zip(present_labels, precision)),\n",
    "        recall_per_class=dict(zip(present_labels, recall)),\n",
    "        f1_per_class=dict(zip(present_labels, f1)),\n",
    "        macro_f1=macro_f1,\n",
    "        weighted_f1=weighted_f1,\n",
    "        cohens_kappa=kappa,\n",
    "        support_per_class=dict(zip(present_labels, support.astype(int))),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def validate_classifications(\n",
    "    test_examples: List[TrainingExample],\n",
    "    classifier: OpennessClassifier,\n",
    "    progress_callback: Optional[callable] = None,\n",
    ") -> ValidationResult:\n",
    "    \"\"\"Validate classifier on test set.\n",
    "    \n",
    "    Classifies all test examples and computes metrics vs ground truth.\n",
    "    \n",
    "    Args:\n",
    "        test_examples: Test examples with ground truth labels\n",
    "        classifier: Trained classifier\n",
    "        progress_callback: Optional callback(processed, total)\n",
    "        \n",
    "    Returns:\n",
    "        ValidationResult with metrics and confusion matrices\n",
    "    \"\"\"\n",
    "    # Separate data and code examples\n",
    "    data_examples = [e for e in test_examples if e.statement_type == ClassificationType.DATA]\n",
    "    code_examples = [e for e in test_examples if e.statement_type == ClassificationType.CODE]\n",
    "    \n",
    "    result = ValidationResult(\n",
    "        test_set_size=len(test_examples),\n",
    "        train_set_size=len(classifier.data_examples) + len(classifier.code_examples),\n",
    "        model_config=classifier.config.llm,\n",
    "    )\n",
    "    \n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    processed = 0\n",
    "    \n",
    "    # Validate data classifications\n",
    "    if data_examples:\n",
    "        data_true, data_pred, data_misclassified = _validate_examples(\n",
    "            data_examples, classifier, ClassificationType.DATA,\n",
    "            lambda p: progress_callback(processed + p, len(test_examples)) if progress_callback else None\n",
    "        )\n",
    "        \n",
    "        if data_true:\n",
    "            result.data_metrics = compute_metrics(data_true, data_pred)\n",
    "            result.confusion_matrices['data'] = confusion_matrix(\n",
    "                data_true, data_pred,\n",
    "                labels=['open', 'mostly_open', 'mostly_closed', 'closed']\n",
    "            )\n",
    "            all_true.extend(data_true)\n",
    "            all_pred.extend(data_pred)\n",
    "            result.misclassified_examples.extend(data_misclassified)\n",
    "        \n",
    "        processed += len(data_examples)\n",
    "    \n",
    "    # Validate code classifications\n",
    "    if code_examples:\n",
    "        code_true, code_pred, code_misclassified = _validate_examples(\n",
    "            code_examples, classifier, ClassificationType.CODE,\n",
    "            lambda p: progress_callback(processed + p, len(test_examples)) if progress_callback else None\n",
    "        )\n",
    "        \n",
    "        if code_true:\n",
    "            result.code_metrics = compute_metrics(code_true, code_pred)\n",
    "            result.confusion_matrices['code'] = confusion_matrix(\n",
    "                code_true, code_pred,\n",
    "                labels=['open', 'mostly_open', 'mostly_closed', 'closed']\n",
    "            )\n",
    "            all_true.extend(code_true)\n",
    "            all_pred.extend(code_pred)\n",
    "            result.misclassified_examples.extend(code_misclassified)\n",
    "    \n",
    "    # Overall accuracy\n",
    "    if all_true:\n",
    "        result.overall_accuracy = accuracy_score(all_true, all_pred)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def _validate_examples(\n",
    "    examples: List[TrainingExample],\n",
    "    classifier: OpennessClassifier,\n",
    "    statement_type: ClassificationType,\n",
    "    progress_callback: Optional[callable] = None,\n",
    ") -> Tuple[List[str], List[str], List[Tuple[str, str, str]]]:\n",
    "    \"\"\"Validate a list of examples.\"\"\"\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    misclassified = []\n",
    "    \n",
    "    for i, ex in enumerate(examples):\n",
    "        try:\n",
    "            result = classifier.classify_statement(\n",
    "                ex.statement_text, \n",
    "                statement_type,\n",
    "                return_reasoning=False\n",
    "            )\n",
    "            \n",
    "            true_label = ex.ground_truth.value\n",
    "            pred_label = result.category.value\n",
    "            \n",
    "            true_labels.append(true_label)\n",
    "            pred_labels.append(pred_label)\n",
    "            \n",
    "            if true_label != pred_label:\n",
    "                misclassified.append((\n",
    "                    ex.statement_text[:100],\n",
    "                    true_label,\n",
    "                    pred_label\n",
    "                ))\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to classify example {ex.id}: {e}\")\n",
    "        \n",
    "        if progress_callback:\n",
    "            progress_callback(i + 1)\n",
    "    \n",
    "    return true_labels, pred_labels, misclassified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def cross_validate(\n",
    "    examples: List[TrainingExample],\n",
    "    config: ClassifierConfig,\n",
    "    n_folds: int = 5,\n",
    "    progress_callback: Optional[callable] = None,\n",
    ") -> List[ValidationResult]:\n",
    "    \"\"\"Perform k-fold cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        examples: All training examples\n",
    "        config: Classifier configuration\n",
    "        n_folds: Number of folds (default: 5)\n",
    "        progress_callback: Optional callback(fold, n_folds)\n",
    "        \n",
    "    Returns:\n",
    "        List of ValidationResult, one per fold\n",
    "    \"\"\"\n",
    "    # Separate by type\n",
    "    data_examples = [e for e in examples if e.statement_type == ClassificationType.DATA]\n",
    "    code_examples = [e for e in examples if e.statement_type == ClassificationType.CODE]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Cross-validate data examples\n",
    "    if data_examples:\n",
    "        data_results = _cross_validate_type(\n",
    "            data_examples, code_examples, config, n_folds,\n",
    "            ClassificationType.DATA\n",
    "        )\n",
    "        results.extend(data_results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def _cross_validate_type(\n",
    "    examples: List[TrainingExample],\n",
    "    other_examples: List[TrainingExample],\n",
    "    config: ClassifierConfig,\n",
    "    n_folds: int,\n",
    "    statement_type: ClassificationType,\n",
    ") -> List[ValidationResult]:\n",
    "    \"\"\"Cross-validate for a single type.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    labels = [e.ground_truth.value for e in examples]\n",
    "    kfold = StratifiedKFold(n_splits=min(n_folds, len(examples)), shuffle=True, random_state=42)\n",
    "    \n",
    "    embedding_model = EmbeddingModel(config.embedding_model)\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kfold.split(examples, labels)):\n",
    "        train_ex = [examples[i] for i in train_idx]\n",
    "        test_ex = [examples[i] for i in test_idx]\n",
    "        \n",
    "        # Compute embeddings\n",
    "        compute_embeddings(train_ex, embedding_model)\n",
    "        \n",
    "        # Create classifier with fold's training data\n",
    "        if statement_type == ClassificationType.DATA:\n",
    "            classifier = OpennessClassifier(\n",
    "                config=config,\n",
    "                data_examples=train_ex,\n",
    "                code_examples=other_examples,\n",
    "                embedding_model=embedding_model,\n",
    "            )\n",
    "        else:\n",
    "            classifier = OpennessClassifier(\n",
    "                config=config,\n",
    "                data_examples=other_examples,\n",
    "                code_examples=train_ex,\n",
    "                embedding_model=embedding_model,\n",
    "            )\n",
    "        \n",
    "        # Validate\n",
    "        result = validate_classifications(test_ex, classifier)\n",
    "        results.append(result)\n",
    "        \n",
    "        logging.info(f\"Fold {fold + 1}/{n_folds}: Accuracy = {result.overall_accuracy:.3f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def performance_comparison(\n",
    "    results: List[ValidationResult],\n",
    "    labels: Optional[List[str]] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compare performance across multiple validation results.\n",
    "    \n",
    "    Useful for comparing model versions or cross-validation folds.\n",
    "    \n",
    "    Args:\n",
    "        results: List of validation results to compare\n",
    "        labels: Optional labels for each result\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with metrics comparison\n",
    "    \"\"\"\n",
    "    if labels is None:\n",
    "        labels = [f\"Run {i+1}\" for i in range(len(results))]\n",
    "    \n",
    "    rows = []\n",
    "    for label, result in zip(labels, results):\n",
    "        row = {'label': label, 'overall_accuracy': result.overall_accuracy}\n",
    "        \n",
    "        if result.data_metrics:\n",
    "            row['data_accuracy'] = result.data_metrics.accuracy\n",
    "            row['data_kappa'] = result.data_metrics.cohens_kappa\n",
    "            row['data_macro_f1'] = result.data_metrics.macro_f1\n",
    "        \n",
    "        if result.code_metrics:\n",
    "            row['code_accuracy'] = result.code_metrics.accuracy\n",
    "            row['code_kappa'] = result.code_metrics.cohens_kappa\n",
    "            row['code_macro_f1'] = result.code_metrics.macro_f1\n",
    "        \n",
    "        rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Add summary statistics\n",
    "    summary = df.describe().loc[['mean', 'std']]\n",
    "    \n",
    "    return df, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metrics computation\n",
    "y_true = ['open', 'open', 'closed', 'closed', 'mostly_open', 'mostly_closed']\n",
    "y_pred = ['open', 'mostly_open', 'closed', 'closed', 'mostly_open', 'closed']\n",
    "\n",
    "metrics = compute_metrics(y_true, y_pred)\n",
    "print(f\"Accuracy: {metrics.accuracy:.3f}\")\n",
    "print(f\"Cohen's Kappa: {metrics.cohens_kappa:.3f}\")\n",
    "print(f\"Macro F1: {metrics.macro_f1:.3f}\")\n",
    "print(\"\\nMarkdown output:\")\n",
    "print(metrics.to_markdown()[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
