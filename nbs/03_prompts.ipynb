{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts Module\n",
    "\n",
    "> Few-shot prompt construction and kNN example selection.\n",
    "\n",
    "This module handles:\n",
    "- kNN-based selection of semantically similar training examples\n",
    "- Few-shot prompt construction with chain-of-thought reasoning\n",
    "- Prompt templates for data and code classification\n",
    "\n",
    "**Research Background**: Few-shot learning with semantic similarity-based example selection substantially improves LLM classification accuracy over random sampling (Brown et al., 2020; Liu et al., 2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "from typing import List, Optional\n",
    "\n",
    "from openness_classifier.core import OpennessCategory, ClassificationType\n",
    "from openness_classifier.data import TrainingExample, EmbeddingModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN Example Selection\n",
    "\n",
    "Select the k most semantically similar training examples for a given statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def select_knn_examples(\n",
    "    statement: str,\n",
    "    training_examples: List[TrainingExample],\n",
    "    embedding_model: EmbeddingModel,\n",
    "    k: int = 5,\n",
    "    diversify: bool = True\n",
    ") -> List[TrainingExample]:\n",
    "    \"\"\"Select k most similar training examples using kNN.\n",
    "    \n",
    "    Uses cosine similarity between sentence embeddings to find\n",
    "    the most relevant examples for few-shot prompting.\n",
    "    \n",
    "    Args:\n",
    "        statement: The statement to classify\n",
    "        training_examples: Pool of training examples with embeddings\n",
    "        embedding_model: Model for computing statement embedding\n",
    "        k: Number of examples to select\n",
    "        diversify: If True, ensure variety in selected examples' labels\n",
    "        \n",
    "    Returns:\n",
    "        List of k most similar training examples\n",
    "    \"\"\"\n",
    "    if not training_examples:\n",
    "        return []\n",
    "    \n",
    "    # Ensure all examples have embeddings\n",
    "    for ex in training_examples:\n",
    "        if ex.embedding is None:\n",
    "            ex.embedding = embedding_model.encode(ex.statement_text)\n",
    "    \n",
    "    # Compute embedding for input statement\n",
    "    statement_embedding = embedding_model.encode(statement)\n",
    "    \n",
    "    # Compute similarities\n",
    "    similarities = []\n",
    "    for ex in training_examples:\n",
    "        sim = _cosine_similarity(statement_embedding, ex.embedding)\n",
    "        similarities.append((ex, sim))\n",
    "    \n",
    "    # Sort by similarity (descending)\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    if diversify:\n",
    "        # Select examples ensuring label diversity\n",
    "        return _select_diverse_examples(similarities, k)\n",
    "    else:\n",
    "        # Just take top k\n",
    "        return [ex for ex, _ in similarities[:k]]\n",
    "\n",
    "\n",
    "def _cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "\n",
    "\n",
    "def _select_diverse_examples(\n",
    "    sorted_examples: List[tuple],\n",
    "    k: int\n",
    ") -> List[TrainingExample]:\n",
    "    \"\"\"Select examples ensuring label diversity.\n",
    "    \n",
    "    Tries to include at least one example from each category\n",
    "    while still prioritizing similarity.\n",
    "    \"\"\"\n",
    "    selected = []\n",
    "    seen_labels = set()\n",
    "    \n",
    "    # First pass: get one example per unique label from top candidates\n",
    "    for ex, sim in sorted_examples:\n",
    "        if ex.ground_truth not in seen_labels:\n",
    "            selected.append(ex)\n",
    "            seen_labels.add(ex.ground_truth)\n",
    "            if len(selected) >= k:\n",
    "                return selected\n",
    "    \n",
    "    # Second pass: fill remaining slots with most similar\n",
    "    for ex, sim in sorted_examples:\n",
    "        if ex not in selected:\n",
    "            selected.append(ex)\n",
    "            if len(selected) >= k:\n",
    "                break\n",
    "    \n",
    "    return selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "Chain-of-thought prompt templates for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert research analyst specializing in evaluating data and code availability statements in scholarly publications. Your task is to classify the openness of availability statements using a 4-category taxonomy.\n",
    "\n",
    "Classification Categories (from most open to least open):\n",
    "\n",
    "1. **open**: Fully accessible with no restrictions\n",
    "   - Data/code in public repository (Zenodo, Figshare, GitHub public)\n",
    "   - No registration, login, or approval required\n",
    "   - Open license (CC-BY, MIT, etc.)\n",
    "\n",
    "2. **mostly_open**: Largely accessible with minor restrictions\n",
    "   - Public repository but requires free registration\n",
    "   - Institutional access (freely available to affiliated researchers)\n",
    "   - Minor conditions (e.g., cite the source)\n",
    "\n",
    "3. **mostly_closed**: Largely restricted with limited access\n",
    "   - Data use agreements required\n",
    "   - Partial availability (some data/code withheld)\n",
    "   - Significant restrictions on use or redistribution\n",
    "   - Available only through specific collaborations\n",
    "\n",
    "4. **closed**: Not accessible\n",
    "   - \"Available upon request\" (regardless of how polite)\n",
    "   - Confidential, proprietary, or restricted\n",
    "   - No statement provided\n",
    "   - Contact author for access\n",
    "\n",
    "IMPORTANT: \"Available upon request\" or \"contact the authors\" is ALWAYS classified as **closed**.\"\"\"\n",
    "\n",
    "\n",
    "DATA_CLASSIFICATION_TEMPLATE = \"\"\"Classify the following DATA availability statement.\n",
    "\n",
    "{few_shot_examples}\n",
    "\n",
    "Now classify this statement:\n",
    "\n",
    "Statement: {statement}\n",
    "\n",
    "Think step-by-step:\n",
    "1. What repository or location is mentioned (if any)?\n",
    "2. What access restrictions are described?\n",
    "3. Is there any \"upon request\" or \"contact author\" language?\n",
    "\n",
    "Based on your analysis, provide:\n",
    "- Classification: [open/mostly_open/mostly_closed/closed]\n",
    "- Confidence: [0.0-1.0]\n",
    "- Reasoning: [brief explanation]\"\"\"\n",
    "\n",
    "\n",
    "CODE_CLASSIFICATION_TEMPLATE = \"\"\"Classify the following CODE availability statement.\n",
    "\n",
    "{few_shot_examples}\n",
    "\n",
    "Now classify this statement:\n",
    "\n",
    "Statement: {statement}\n",
    "\n",
    "Think step-by-step:\n",
    "1. What repository or platform is mentioned (if any)?\n",
    "2. Is the code publicly accessible?\n",
    "3. Are there any restrictions on access or use?\n",
    "\n",
    "Based on your analysis, provide:\n",
    "- Classification: [open/mostly_open/mostly_closed/closed]\n",
    "- Confidence: [0.0-1.0]\n",
    "- Reasoning: [brief explanation]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def build_few_shot_prompt(\n",
    "    statement: str,\n",
    "    statement_type: ClassificationType,\n",
    "    examples: List[TrainingExample],\n",
    "    include_reasoning: bool = True\n",
    ") -> str:\n",
    "    \"\"\"Build a few-shot classification prompt.\n",
    "    \n",
    "    Args:\n",
    "        statement: The statement to classify\n",
    "        statement_type: DATA or CODE\n",
    "        examples: Selected few-shot examples\n",
    "        include_reasoning: Whether to include CoT reasoning template\n",
    "        \n",
    "    Returns:\n",
    "        Complete prompt string\n",
    "    \"\"\"\n",
    "    # Format examples\n",
    "    example_strs = []\n",
    "    for i, ex in enumerate(examples, 1):\n",
    "        example_strs.append(\n",
    "            f\"Example {i}:\\n\"\n",
    "            f\"Statement: {ex.statement_text}\\n\"\n",
    "            f\"Classification: {ex.ground_truth.value}\"\n",
    "        )\n",
    "    \n",
    "    few_shot_block = \"\\n\\n\".join(example_strs)\n",
    "    \n",
    "    if few_shot_block:\n",
    "        few_shot_block = f\"Here are some examples:\\n\\n{few_shot_block}\\n\"\n",
    "    \n",
    "    # Select template\n",
    "    if statement_type == ClassificationType.DATA:\n",
    "        template = DATA_CLASSIFICATION_TEMPLATE\n",
    "    else:\n",
    "        template = CODE_CLASSIFICATION_TEMPLATE\n",
    "    \n",
    "    return template.format(\n",
    "        few_shot_examples=few_shot_block,\n",
    "        statement=statement\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_classification_response(response: str) -> tuple:\n",
    "    \"\"\"Parse LLM response to extract classification, confidence, and reasoning.\n",
    "    \n",
    "    Args:\n",
    "        response: Raw LLM response text\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (OpennessCategory, confidence_score, reasoning)\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Default values\n",
    "    category = None\n",
    "    confidence = 0.8\n",
    "    reasoning = response\n",
    "    \n",
    "    # Try to extract classification\n",
    "    class_match = re.search(\n",
    "        r'Classification:\\s*(open|mostly_open|mostly_closed|closed)',\n",
    "        response,\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    if class_match:\n",
    "        category = OpennessCategory.from_string(class_match.group(1))\n",
    "    else:\n",
    "        # Try alternative patterns\n",
    "        for cat in ['open', 'mostly_open', 'mostly_closed', 'closed']:\n",
    "            if cat in response.lower():\n",
    "                category = OpennessCategory.from_string(cat)\n",
    "                break\n",
    "    \n",
    "    # Try to extract confidence\n",
    "    conf_match = re.search(r'Confidence:\\s*([0-9.]+)', response, re.IGNORECASE)\n",
    "    if conf_match:\n",
    "        try:\n",
    "            confidence = float(conf_match.group(1))\n",
    "            confidence = max(0.0, min(1.0, confidence))  # Clamp to [0, 1]\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    # Try to extract reasoning\n",
    "    reason_match = re.search(r'Reasoning:\\s*(.+?)(?=$|Classification:|Confidence:)', \n",
    "                            response, re.IGNORECASE | re.DOTALL)\n",
    "    if reason_match:\n",
    "        reasoning = reason_match.group(1).strip()\n",
    "    \n",
    "    if category is None:\n",
    "        # Default to closed if we can't parse\n",
    "        category = OpennessCategory.CLOSED\n",
    "        confidence = 0.3  # Low confidence for unparseable response\n",
    "        reasoning = f\"Could not parse response: {response[:200]}...\"\n",
    "    \n",
    "    return category, confidence, reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompt construction\n",
    "from openness_classifier.core import OpennessCategory, ClassificationType\n",
    "\n",
    "# Create mock examples\n",
    "class MockExample:\n",
    "    def __init__(self, text, label):\n",
    "        self.statement_text = text\n",
    "        self.ground_truth = OpennessCategory.from_string(label)\n",
    "\n",
    "examples = [\n",
    "    MockExample(\"Data available at https://zenodo.org/record/12345\", \"open\"),\n",
    "    MockExample(\"Data available upon request from the authors\", \"closed\"),\n",
    "]\n",
    "\n",
    "prompt = build_few_shot_prompt(\n",
    "    \"Data are deposited in Figshare at doi:10.6084/m9.figshare.12345\",\n",
    "    ClassificationType.DATA,\n",
    "    examples\n",
    ")\n",
    "\n",
    "print(prompt[:500])\n",
    "print(\"...\")\n",
    "print(\"Prompt construction test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test response parsing\n",
    "test_response = \"\"\"Let me analyze this statement step by step.\n",
    "\n",
    "1. The data is stored in Zenodo, which is a public repository.\n",
    "2. No access restrictions are mentioned.\n",
    "3. No \"upon request\" language.\n",
    "\n",
    "Classification: open\n",
    "Confidence: 0.95\n",
    "Reasoning: Data is deposited in a public repository (Zenodo) with a DOI, indicating full open access.\"\"\"\n",
    "\n",
    "category, confidence, reasoning = parse_classification_response(test_response)\n",
    "print(f\"Category: {category.value}\")\n",
    "print(f\"Confidence: {confidence}\")\n",
    "print(f\"Reasoning: {reasoning}\")\n",
    "print(\"Response parsing test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
