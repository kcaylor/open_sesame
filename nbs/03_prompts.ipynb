{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts Module\n",
    "\n",
    "> Few-shot prompt construction and kNN example selection.\n",
    "\n",
    "This module handles:\n",
    "- kNN-based selection of semantically similar training examples\n",
    "- Few-shot prompt construction with chain-of-thought reasoning\n",
    "- Prompt templates for data and code classification\n",
    "\n",
    "**Research Background**: Few-shot learning with semantic similarity-based example selection substantially improves LLM classification accuracy over random sampling (Brown et al., 2020; Liu et al., 2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "from typing import List, Optional\n",
    "\n",
    "from openness_classifier.core import OpennessCategory, ClassificationType\n",
    "from openness_classifier.data import TrainingExample, EmbeddingModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN Example Selection\n",
    "\n",
    "Select the k most semantically similar training examples for a given statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def select_knn_examples(\n",
    "    statement: str,\n",
    "    training_examples: List[TrainingExample],\n",
    "    embedding_model: EmbeddingModel,\n",
    "    k: int = 5,\n",
    "    diversify: bool = True\n",
    ") -> List[TrainingExample]:\n",
    "    \"\"\"Select k most similar training examples using kNN.\n",
    "    \n",
    "    Uses cosine similarity between sentence embeddings to find\n",
    "    the most relevant examples for few-shot prompting.\n",
    "    \n",
    "    Args:\n",
    "        statement: The statement to classify\n",
    "        training_examples: Pool of training examples with embeddings\n",
    "        embedding_model: Model for computing statement embedding\n",
    "        k: Number of examples to select\n",
    "        diversify: If True, ensure variety in selected examples' labels\n",
    "        \n",
    "    Returns:\n",
    "        List of k most similar training examples\n",
    "    \"\"\"\n",
    "    if not training_examples:\n",
    "        return []\n",
    "    \n",
    "    # Ensure all examples have embeddings\n",
    "    for ex in training_examples:\n",
    "        if ex.embedding is None:\n",
    "            ex.embedding = embedding_model.encode(ex.statement_text)\n",
    "    \n",
    "    # Compute embedding for input statement\n",
    "    statement_embedding = embedding_model.encode(statement)\n",
    "    \n",
    "    # Compute similarities\n",
    "    similarities = []\n",
    "    for ex in training_examples:\n",
    "        sim = _cosine_similarity(statement_embedding, ex.embedding)\n",
    "        similarities.append((ex, sim))\n",
    "    \n",
    "    # Sort by similarity (descending)\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    if diversify:\n",
    "        # Select examples ensuring label diversity\n",
    "        return _select_diverse_examples(similarities, k)\n",
    "    else:\n",
    "        # Just take top k\n",
    "        return [ex for ex, _ in similarities[:k]]\n",
    "\n",
    "\n",
    "def _cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "\n",
    "\n",
    "def _select_diverse_examples(\n",
    "    sorted_examples: List[tuple],\n",
    "    k: int\n",
    ") -> List[TrainingExample]:\n",
    "    \"\"\"Select examples ensuring label diversity.\n",
    "    \n",
    "    Tries to include at least one example from each category\n",
    "    while still prioritizing similarity.\n",
    "    \"\"\"\n",
    "    selected = []\n",
    "    seen_labels = set()\n",
    "    \n",
    "    # First pass: get one example per unique label from top candidates\n",
    "    for ex, sim in sorted_examples:\n",
    "        if ex.ground_truth not in seen_labels:\n",
    "            selected.append(ex)\n",
    "            seen_labels.add(ex.ground_truth)\n",
    "            if len(selected) >= k:\n",
    "                return selected\n",
    "    \n",
    "    # Second pass: fill remaining slots with most similar\n",
    "    for ex, sim in sorted_examples:\n",
    "        if ex not in selected:\n",
    "            selected.append(ex)\n",
    "            if len(selected) >= k:\n",
    "                break\n",
    "    \n",
    "    return selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## Completeness Indicators and Prompt Templates\n\nThe refined taxonomy (002-refine-classification-taxonomy) introduces:\n1. **Completeness indicators** for distinguishing mostly_open from mostly_closed\n2. **5-step chain-of-thought reasoning** for more accurate boundary classification\n3. **Hard precedence rule (FR-004)** for substantial access barriers"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#| export\n# Completeness indicators for mostly_open classification (FR-002, FR-003, refined per research.md)\nMOSTLY_OPEN_DATA_COMPLETENESS = [\n    \"All\",\n    \"Raw; Results; Source Data\",\n    \"Raw; Results\",\n    \"Raw\",\n    \"Raw; Source Data\",\n]\n\nMOSTLY_OPEN_CODE_COMPLETENESS = [\n    \"All\",\n    \"Model\",\n    \"Models\",\n    \"Download; Process; Analysis; Figures\",\n    \"Processing; Generate Results\",\n    \"Processing; Results\",\n    \"Models; Results\",\n    \"Models; Analysis\",\n    \"Analysis; Figures\",\n    \"Model; Figures\",\n    \"Results; Figures\",\n    \"Generate Results; Figures\",\n]\n\n# Substantial access barriers that force mostly_closed (FR-004 hard precedence rule)\nSUBSTANTIAL_BARRIERS = [\n    \"data use agreement\",\n    \"confidentiality\",\n    \"proprietary\",\n    \"upon request\",\n    \"contact author\",\n    \"restricted access\",\n    \"DUA\",\n    \"confidential\",\n]\n\n\nSYSTEM_PROMPT = \"\"\"You are an expert research analyst specializing in evaluating data and code availability statements in scholarly publications. Your task is to classify the openness of availability statements using a refined 4-category taxonomy.\n\nClassification Categories (from most open to least open):\n\n1. **open**: Fully accessible with no restrictions\n   - Data/code in public repository (Zenodo, Figshare, GitHub public)\n   - No registration, login, or approval required\n   - Open license (CC-BY, MIT, etc.)\n\n2. **mostly_open**: Largely accessible with minor restrictions\n   - HIGH COMPLETENESS: Most or all data/code types available\n     * Data: All, Raw, Raw; Results, Raw; Results; Source Data\n     * Code: All, Models, Download; Process; Analysis; Figures, or similar comprehensive combinations\n   - MINOR BARRIERS ONLY: Free registration, institutional access, citation requirement\n   - Can use non-persistent repository (GitHub) IF comprehensive materials provided\n\n3. **mostly_closed**: Largely restricted with limited access\n   - LOW/PARTIAL COMPLETENESS: Only some data/code types available (e.g., Results only, Processing scripts only)\n   - SUBSTANTIAL BARRIERS: Data use agreements, confidentiality restrictions, proprietary terms\n   - Available only through specific collaborations or agreements\n   - Note: Substantial barriers ALWAYS force mostly_closed regardless of completeness\n\n4. **closed**: Not accessible\n   - \"Available upon request\" (regardless of how polite)\n   - Confidential, proprietary, or restricted\n   - No statement provided\n   - Contact author for access\n\nCRITICAL RULE (HARD PRECEDENCE - FR-004):\nIf substantial access barriers exist (data use agreements, proprietary terms, confidentiality restrictions, \"available upon request\"), the classification MUST be mostly_closed or closed, REGARDLESS of completeness or repository quality. This rule has absolute precedence.\n\nIMPORTANT: \"Available upon request\" or \"contact the authors\" is ALWAYS classified as **closed**.\"\"\"\n\n\nDATA_CLASSIFICATION_TEMPLATE = \"\"\"Classify the following DATA availability statement.\n\n{few_shot_examples}\n\nNow classify this statement:\n\nStatement: {statement}\n\nThink step-by-step through this 5-step reasoning process:\n\n1. **Identify data types mentioned**: What types of data are available?\n   - Look for: Raw data, Results, Source Data, Processed data, All data\n   - HIGH completeness indicators: All, Raw; Results; Source Data, Raw; Results, Raw\n\n2. **Assess completeness**: Does the statement indicate all necessary materials for reproduction or only partial materials?\n   - All/most data types = HIGH completeness → favors mostly_open\n   - Only some data types (e.g., \"Results only\") = LOW completeness → favors mostly_closed\n\n3. **Identify access barriers**: What restrictions are mentioned?\n   - MINOR barriers (allow mostly_open): Free registration, institutional access, citation required\n   - SUBSTANTIAL barriers (force mostly_closed): Data use agreement, confidentiality, proprietary, \"upon request\"\n\n4. **Determine repository type** (if mentioned):\n   - PERSISTENT (Zenodo, Figshare, Dryad, DOI): Adds confidence to classification\n   - NON-PERSISTENT (GitHub, personal website): Acceptable IF high completeness\n\n5. **Apply classification rules** (in this order):\n   - FIRST CHECK: Substantial barrier present? → mostly_closed or closed (STOP, do not consider other factors)\n   - \"Upon request\" or \"contact author\"? → closed\n   - No substantial barrier + HIGH completeness → mostly_open\n   - No substantial barrier + LOW completeness → mostly_closed\n   - No barriers + fully public → open\n\nBased on your analysis, provide:\n- Classification: [open/mostly_open/mostly_closed/closed]\n- Confidence: [0.0-1.0]\n- Reasoning: [Your step-by-step analysis mentioning specific data types, barriers, and repository]\"\"\"\n\n\nCODE_CLASSIFICATION_TEMPLATE = \"\"\"Classify the following CODE availability statement.\n\n{few_shot_examples}\n\nNow classify this statement:\n\nStatement: {statement}\n\nThink step-by-step through this 5-step reasoning process:\n\n1. **Identify code types mentioned**: What types of code are available?\n   - Look for: Download scripts, Processing scripts, Analysis code, Figure generation, Models, All code\n   - HIGH completeness indicators: All, Models, Download; Process; Analysis; Figures, Processing; Generate Results\n\n2. **Assess completeness**: Does the statement indicate all necessary code for reproduction or only partial code?\n   - All/most code types = HIGH completeness → favors mostly_open\n   - Only some code types (e.g., \"Processing scripts only\") = LOW completeness → favors mostly_closed\n\n3. **Identify access barriers**: What restrictions are mentioned?\n   - MINOR barriers (allow mostly_open): Free registration, institutional access, citation required\n   - SUBSTANTIAL barriers (force mostly_closed): Proprietary code, confidential algorithms, \"upon request\"\n\n4. **Determine repository type** (if mentioned):\n   - PERSISTENT (Zenodo, Figshare with DOI): Adds confidence to classification\n   - NON-PERSISTENT (GitHub, GitLab): Acceptable IF comprehensive code provided (all types)\n\n5. **Apply classification rules** (in this order):\n   - FIRST CHECK: Substantial barrier present? → mostly_closed or closed (STOP, do not consider other factors)\n   - \"Upon request\" or \"contact author\"? → closed\n   - No substantial barrier + HIGH completeness → mostly_open\n   - No substantial barrier + LOW completeness → mostly_closed\n   - No barriers + fully public → open\n\nBased on your analysis, provide:\n- Classification: [open/mostly_open/mostly_closed/closed]\n- Confidence: [0.0-1.0]\n- Reasoning: [Your step-by-step analysis mentioning specific code types, barriers, and repository]\"\"\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def build_few_shot_prompt(\n",
    "    statement: str,\n",
    "    statement_type: ClassificationType,\n",
    "    examples: List[TrainingExample],\n",
    "    include_reasoning: bool = True\n",
    ") -> str:\n",
    "    \"\"\"Build a few-shot classification prompt.\n",
    "    \n",
    "    Args:\n",
    "        statement: The statement to classify\n",
    "        statement_type: DATA or CODE\n",
    "        examples: Selected few-shot examples\n",
    "        include_reasoning: Whether to include CoT reasoning template\n",
    "        \n",
    "    Returns:\n",
    "        Complete prompt string\n",
    "    \"\"\"\n",
    "    # Format examples\n",
    "    example_strs = []\n",
    "    for i, ex in enumerate(examples, 1):\n",
    "        example_strs.append(\n",
    "            f\"Example {i}:\\n\"\n",
    "            f\"Statement: {ex.statement_text}\\n\"\n",
    "            f\"Classification: {ex.ground_truth.value}\"\n",
    "        )\n",
    "    \n",
    "    few_shot_block = \"\\n\\n\".join(example_strs)\n",
    "    \n",
    "    if few_shot_block:\n",
    "        few_shot_block = f\"Here are some examples:\\n\\n{few_shot_block}\\n\"\n",
    "    \n",
    "    # Select template\n",
    "    if statement_type == ClassificationType.DATA:\n",
    "        template = DATA_CLASSIFICATION_TEMPLATE\n",
    "    else:\n",
    "        template = CODE_CLASSIFICATION_TEMPLATE\n",
    "    \n",
    "    return template.format(\n",
    "        few_shot_examples=few_shot_block,\n",
    "        statement=statement\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef has_substantial_barrier(text: str) -> bool:\n    \"\"\"Check if text contains any substantial access barrier (FR-004).\n\n    Args:\n        text: Statement or reasoning text to check\n\n    Returns:\n        True if substantial barrier found, False otherwise\n    \"\"\"\n    text_lower = text.lower()\n    return any(barrier.lower() in text_lower for barrier in SUBSTANTIAL_BARRIERS)\n\n\ndef extract_completeness_attributes(\n    reasoning: str,\n    statement_type: ClassificationType\n) -> dict:\n    \"\"\"Extract completeness attributes from LLM reasoning (FR-007, SC-004).\n\n    Parses the 5-step reasoning to identify data/code types, barriers,\n    and repository type. Used for validation and audit.\n\n    Args:\n        reasoning: The LLM's reasoning text\n        statement_type: DATA or CODE\n\n    Returns:\n        Dictionary with extracted attributes:\n        - types_mentioned: List of data/code types identified\n        - completeness_level: 'high' or 'low' based on types\n        - barriers_found: List of barriers mentioned\n        - substantial_barrier: Boolean (FR-004)\n        - repository_type: 'persistent', 'non_persistent', or 'unknown'\n        - reasoning_quality: Score 0-1 based on attribute coverage\n    \"\"\"\n    import re\n    reasoning_lower = reasoning.lower()\n\n    # Extract data/code types mentioned\n    types_mentioned = []\n    if statement_type == ClassificationType.DATA:\n        type_patterns = ['raw', 'results', 'source data', 'processed', 'all data', 'raw data']\n    else:\n        type_patterns = ['download', 'processing', 'analysis', 'figures', 'models?', 'all code']\n\n    for pattern in type_patterns:\n        if re.search(pattern, reasoning_lower):\n            types_mentioned.append(pattern.replace('?', ''))\n\n    # Determine completeness level\n    high_completeness_keywords = ['high completeness', 'comprehensive', 'all', 'complete', 'most']\n    low_completeness_keywords = ['low completeness', 'partial', 'only', 'limited', 'some']\n\n    completeness_level = 'unknown'\n    if any(kw in reasoning_lower for kw in high_completeness_keywords):\n        completeness_level = 'high'\n    elif any(kw in reasoning_lower for kw in low_completeness_keywords):\n        completeness_level = 'low'\n\n    # Extract barriers found\n    barriers_found = [b for b in SUBSTANTIAL_BARRIERS if b.lower() in reasoning_lower]\n\n    # Check for substantial barrier\n    substantial_barrier = has_substantial_barrier(reasoning)\n\n    # Determine repository type\n    persistent_repos = ['zenodo', 'figshare', 'dryad', 'doi:']\n    non_persistent_repos = ['github', 'gitlab', 'personal website', 'supplementary']\n\n    repository_type = 'unknown'\n    if any(repo in reasoning_lower for repo in persistent_repos):\n        repository_type = 'persistent'\n    elif any(repo in reasoning_lower for repo in non_persistent_repos):\n        repository_type = 'non_persistent'\n\n    # Calculate reasoning quality score (SC-004: 90% should mention completeness)\n    quality_checks = [\n        len(types_mentioned) > 0,  # Mentions specific types\n        completeness_level != 'unknown',  # Assesses completeness\n        repository_type != 'unknown' or substantial_barrier,  # Mentions repo or barrier\n        'step' in reasoning_lower or any(str(i) in reasoning for i in range(1, 6)),  # Shows reasoning steps\n    ]\n    reasoning_quality = sum(quality_checks) / len(quality_checks)\n\n    return {\n        'types_mentioned': types_mentioned,\n        'completeness_level': completeness_level,\n        'barriers_found': barriers_found,\n        'substantial_barrier': substantial_barrier,\n        'repository_type': repository_type,\n        'reasoning_quality': reasoning_quality,\n    }\n\n\ndef parse_classification_response(response: str) -> tuple:\n    \"\"\"Parse LLM response to extract classification, confidence, and reasoning.\n\n    Enhanced to extract completeness attributes from 5-step reasoning (T007).\n\n    Args:\n        response: Raw LLM response text\n\n    Returns:\n        Tuple of (OpennessCategory, confidence_score, reasoning)\n    \"\"\"\n    import re\n\n    # Default values\n    category = None\n    confidence = 0.8\n    reasoning = response\n\n    # Try to extract classification\n    class_match = re.search(\n        r'Classification:\\s*(open|mostly_open|mostly_closed|closed)',\n        response,\n        re.IGNORECASE\n    )\n    if class_match:\n        category = OpennessCategory.from_string(class_match.group(1))\n    else:\n        # Try alternative patterns\n        for cat in ['open', 'mostly_open', 'mostly_closed', 'closed']:\n            if cat in response.lower():\n                category = OpennessCategory.from_string(cat)\n                break\n\n    # Try to extract confidence\n    conf_match = re.search(r'Confidence:\\s*([0-9.]+)', response, re.IGNORECASE)\n    if conf_match:\n        try:\n            confidence = float(conf_match.group(1))\n            confidence = max(0.0, min(1.0, confidence))  # Clamp to [0, 1]\n        except ValueError:\n            pass\n\n    # Try to extract reasoning\n    reason_match = re.search(r'Reasoning:\\s*(.+?)(?=$|Classification:|Confidence:)',\n                            response, re.IGNORECASE | re.DOTALL)\n    if reason_match:\n        reasoning = reason_match.group(1).strip()\n\n    if category is None:\n        # Default to closed if we can't parse\n        category = OpennessCategory.CLOSED\n        confidence = 0.3  # Low confidence for unparseable response\n        reasoning = f\"Could not parse response: {response[:200]}...\"\n\n    return category, confidence, reasoning"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test prompt construction with refined taxonomy examples\nfrom openness_classifier.core import OpennessCategory, ClassificationType\n\n# Create mock examples demonstrating the refined taxonomy\nclass MockExample:\n    def __init__(self, text, label):\n        self.statement_text = text\n        self.ground_truth = OpennessCategory.from_string(label)\n\n# Examples representing boundary cases for the refined taxonomy\nexamples = [\n    MockExample(\"All raw data and analysis code are available at https://zenodo.org/record/12345\", \"open\"),\n    MockExample(\"Raw; Results; Source Data available at Figshare with free registration\", \"mostly_open\"),\n    MockExample(\"Results available upon completion of data use agreement from ICPSR\", \"mostly_closed\"),\n    MockExample(\"Data available upon reasonable request from the corresponding author\", \"closed\"),\n]\n\n# Test with a boundary case statement\nprompt = build_few_shot_prompt(\n    \"Raw and processed data are deposited in Dryad with open access. Code is on GitHub.\",\n    ClassificationType.DATA,\n    examples\n)\n\nprint(\"=== Refined Taxonomy Prompt Test ===\")\nprint(prompt[:800])\nprint(\"...\")\nprint(\"\\nPrompt construction test passed!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test response parsing with 5-step reasoning\ntest_response = \"\"\"Let me analyze this statement using the 5-step reasoning process.\n\n1. **Identify data types**: The statement mentions raw data and processed data, indicating HIGH completeness.\n\n2. **Assess completeness**: Raw and processed data are comprehensive - HIGH completeness.\n\n3. **Identify access barriers**: \"Open access\" indicates NO substantial barriers.\n\n4. **Repository type**: Dryad is a PERSISTENT repository with DOI.\n\n5. **Apply classification rules**: No substantial barriers + high completeness = mostly_open\n\nClassification: mostly_open\nConfidence: 0.90\nReasoning: High completeness (raw and processed data) in persistent repository (Dryad) with open access makes this mostly_open.\"\"\"\n\n# Parse the response\ncategory, confidence, reasoning = parse_classification_response(test_response)\nprint(f\"Category: {category.value}\")\nprint(f\"Confidence: {confidence}\")\nprint(f\"Reasoning snippet: {reasoning[:100]}...\")\n\n# Test completeness attribute extraction\nattributes = extract_completeness_attributes(test_response, ClassificationType.DATA)\nprint(f\"\\n=== Extracted Completeness Attributes ===\")\nprint(f\"Types mentioned: {attributes['types_mentioned']}\")\nprint(f\"Completeness level: {attributes['completeness_level']}\")\nprint(f\"Substantial barrier: {attributes['substantial_barrier']}\")\nprint(f\"Repository type: {attributes['repository_type']}\")\nprint(f\"Reasoning quality: {attributes['reasoning_quality']:.2f}\")\n\n# Test substantial barrier detection\nassert has_substantial_barrier(\"Data available via data use agreement\") == True\nassert has_substantial_barrier(\"Data available with registration\") == False\nprint(\"\\nAll tests passed!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}