{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Validation and Analysis\n",
    "\n",
    "This tutorial demonstrates how to evaluate classifier performance and generate publication-quality metrics and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openness_classifier.validation import (\n",
    "    validate_classifications,\n",
    "    cross_validate,\n",
    "    compute_metrics,\n",
    "    performance_comparison,\n",
    ")\n",
    "from openness_classifier.visualization import (\n",
    "    plot_confusion_matrix,\n",
    "    plot_validation_results,\n",
    "    plot_class_distribution,\n",
    ")\n",
    "from openness_classifier.data import load_training_data, train_test_split, validate_training_data\n",
    "from openness_classifier.classifier import OpennessClassifier\n",
    "from openness_classifier.config import load_config\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Analyze Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('../../resources/abpoll-open-b71bd12/data/processed/articles_reviewed.csv')\n",
    "\n",
    "if data_path.exists():\n",
    "    data_examples, code_examples = load_training_data(data_path)\n",
    "    \n",
    "    print(f\"Data examples: {len(data_examples)}\")\n",
    "    print(f\"Code examples: {len(code_examples)}\")\n",
    "    \n",
    "    # Validate training data\n",
    "    validation = validate_training_data(data_examples)\n",
    "    print(f\"\\nData Quality Check:\")\n",
    "    print(f\"  Valid: {validation['valid']}\")\n",
    "    print(f\"  Class distribution: {validation['class_distribution']}\")\n",
    "    if validation['warnings']:\n",
    "        print(f\"  Warnings: {validation['warnings']}\")\n",
    "else:\n",
    "    print(f\"Data not found at {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split Data and Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_path.exists():\n",
    "    # Train/test split\n",
    "    train_data, test_data = train_test_split(data_examples, test_size=0.2)\n",
    "    \n",
    "    print(f\"Training set: {len(train_data)} examples\")\n",
    "    print(f\"Test set: {len(test_data)} examples\")\n",
    "    \n",
    "    # Create classifier\n",
    "    config = load_config()\n",
    "    classifier = OpennessClassifier.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'test_data' in dir() and test_data:\n",
    "    print(\"Running validation on test set...\")\n",
    "    \n",
    "    result = validate_classifications(\n",
    "        test_examples=test_data,\n",
    "        classifier=classifier,\n",
    "        progress_callback=lambda p, t: print(f\"\\r{p}/{t}\", end=\"\")\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n\\nValidation Complete!\")\n",
    "    print(f\"Overall Accuracy: {result.overall_accuracy:.1%}\")\n",
    "    \n",
    "    if result.data_metrics:\n",
    "        print(f\"\\nData Classification Metrics:\")\n",
    "        print(f\"  Accuracy: {result.data_metrics.accuracy:.3f}\")\n",
    "        print(f\"  Cohen's Kappa: {result.data_metrics.cohens_kappa:.3f}\")\n",
    "        print(f\"  Macro F1: {result.data_metrics.macro_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'result' in dir() and result.confusion_matrices:\n",
    "    # Plot confusion matrix\n",
    "    if 'data' in result.confusion_matrices:\n",
    "        fig = plot_confusion_matrix(\n",
    "            result.confusion_matrices['data'],\n",
    "            title='Data Availability Classification',\n",
    "            normalize=True\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export Results for Manuscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'result' in dir():\n",
    "    # Generate markdown for manuscript\n",
    "    markdown = result.to_markdown()\n",
    "    print(markdown)\n",
    "    \n",
    "    # Save to file\n",
    "    Path('../../data/validation_results.md').write_text(markdown)\n",
    "    \n",
    "    # Export JSON for archiving\n",
    "    result.to_json('../../data/validation_results.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key metrics:\n",
    "- **Accuracy**: Overall classification accuracy\n",
    "- **Cohen's Kappa**: Inter-rater agreement (target: >0.6)\n",
    "- **Macro F1**: Average F1 across classes\n",
    "- **Per-class precision/recall**: Performance by category\n",
    "\n",
    "Export formats:\n",
    "- `to_markdown()`: Tables for manuscripts\n",
    "- `to_json()`: Full results for archiving\n",
    "- `plot_confusion_matrix()`: Publication-quality figures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
