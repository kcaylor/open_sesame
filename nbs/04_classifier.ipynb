{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type># Classifier Module\n\n> Core classification functions for data and code availability statements.\n\nThis module provides:\n- `classify_statement()`: Classify a single availability statement\n- `classify_publication()`: Classify both data and code for a publication\n- `validate_classification_precedence()`: Enforce FR-004 hard precedence rule\n\nThe classifier uses few-shot learning with semantically similar examples selected via kNN.\n\n**Refined Taxonomy (002-refine-classification-taxonomy)**:\n- 5-step chain-of-thought reasoning for boundary classification\n- Hard precedence rule: substantial barriers â†’ mostly_closed regardless of completeness\n- Post-classification validation to ensure rule enforcement"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#| export\nfrom __future__ import annotations\nfrom typing import Optional, List, Tuple\nfrom datetime import datetime\nimport logging\n\nfrom openness_classifier.core import (\n    OpennessCategory,\n    ClassificationType,\n    Classification,\n    LLMConfiguration,\n    LLMProvider,\n    ClassificationLogger,\n    ClassificationError,\n    LLMError,\n)\nfrom openness_classifier.config import ClassifierConfig, load_config\nfrom openness_classifier.data import (\n    TrainingExample,\n    Publication,\n    EmbeddingModel,\n    load_training_data,\n    compute_embeddings,\n)\nfrom openness_classifier.prompts import (\n    select_knn_examples,\n    build_few_shot_prompt,\n    parse_classification_response,\n    extract_completeness_attributes,\n    has_substantial_barrier,\n    SYSTEM_PROMPT,\n    SUBSTANTIAL_BARRIERS,\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Class\n",
    "\n",
    "Main classifier that manages training data, embeddings, and LLM calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#| export\nclass OpennessClassifier:\n    \"\"\"Few-shot LLM classifier for data and code openness.\n\n    Manages training examples, embeddings, and LLM inference.\n    Implements refined taxonomy with 5-step CoT and FR-004 hard precedence rule.\n\n    Example:\n        >>> classifier = OpennessClassifier.from_config(load_config())\n        >>> result = classifier.classify_statement(\n        ...     \"Data available at https://zenodo.org/record/12345\",\n        ...     ClassificationType.DATA\n        ... )\n        >>> print(result.category)  # OpennessCategory.OPEN\n    \"\"\"\n\n    def __init__(\n        self,\n        config: ClassifierConfig,\n        data_examples: List[TrainingExample],\n        code_examples: List[TrainingExample],\n        embedding_model: EmbeddingModel,\n        logger: Optional[ClassificationLogger] = None,\n    ):\n        self.config = config\n        self.data_examples = data_examples\n        self.code_examples = code_examples\n        self.embedding_model = embedding_model\n        self.llm_provider = LLMProvider(config.llm)\n        self.logger = logger\n\n    @classmethod\n    def from_config(cls, config: ClassifierConfig) -> 'OpennessClassifier':\n        \"\"\"Create classifier from configuration.\n\n        Loads training data and computes embeddings.\n        \"\"\"\n        # Load training data\n        data_examples, code_examples = load_training_data(config.training_data_path)\n\n        # Initialize embedding model\n        embedding_model = EmbeddingModel(config.embedding_model)\n\n        # Compute embeddings\n        compute_embeddings(data_examples, embedding_model)\n        compute_embeddings(code_examples, embedding_model)\n\n        # Setup logger\n        log_path = config.log_dir / f\"classifications_{datetime.now().strftime('%Y%m%d')}.jsonl\"\n        logger = ClassificationLogger(log_path)\n\n        return cls(\n            config=config,\n            data_examples=data_examples,\n            code_examples=code_examples,\n            embedding_model=embedding_model,\n            logger=logger,\n        )\n\n    def classify_statement(\n        self,\n        statement: str,\n        statement_type: ClassificationType,\n        return_reasoning: bool = True,\n        publication_id: Optional[str] = None,\n        enforce_precedence: bool = True,\n    ) -> Classification:\n        \"\"\"Classify a single availability statement.\n\n        Implements the refined taxonomy with 5-step CoT reasoning and\n        hard precedence rule enforcement (FR-004).\n\n        Args:\n            statement: The availability statement text\n            statement_type: DATA or CODE\n            return_reasoning: Include chain-of-thought reasoning\n            publication_id: Optional ID for logging\n            enforce_precedence: If True, apply FR-004 hard precedence rule\n                               post-classification to validate/correct\n\n        Returns:\n            Classification result with category, confidence, and reasoning\n        \"\"\"\n        # Select appropriate training examples\n        examples = (self.data_examples if statement_type == ClassificationType.DATA\n                   else self.code_examples)\n\n        # Select kNN examples\n        selected = select_knn_examples(\n            statement=statement,\n            training_examples=examples,\n            embedding_model=self.embedding_model,\n            k=self.config.few_shot_k,\n        )\n\n        # Build prompt\n        prompt = build_few_shot_prompt(\n            statement=statement,\n            statement_type=statement_type,\n            examples=selected,\n            include_reasoning=return_reasoning,\n        )\n\n        # Prepend system prompt\n        full_prompt = f\"{SYSTEM_PROMPT}\\n\\n{prompt}\"\n\n        # Call LLM\n        response = self.llm_provider.complete(full_prompt)\n\n        # Parse response\n        category, confidence, reasoning = parse_classification_response(response)\n\n        # Apply FR-004 hard precedence rule if enabled (T014)\n        original_category = category\n        if enforce_precedence:\n            category, precedence_applied = validate_classification_precedence(\n                category=category,\n                statement=statement,\n                reasoning=reasoning or \"\",\n            )\n            if precedence_applied:\n                logging.info(\n                    f\"FR-004 precedence rule applied: {original_category.value} -> {category.value} \"\n                    f\"(substantial barrier detected in statement)\"\n                )\n                # Adjust reasoning to note the precedence application\n                if reasoning:\n                    reasoning = (\n                        f\"{reasoning}\\n\\n[VALIDATION NOTE: FR-004 precedence rule applied - \"\n                        f\"substantial access barrier detected, classification adjusted from \"\n                        f\"{original_category.value} to {category.value}]\"\n                    )\n\n        # Create classification result\n        classification = Classification(\n            category=category,\n            statement_type=statement_type,\n            confidence_score=confidence,\n            reasoning=reasoning if return_reasoning else None,\n            model_config=self.config.llm,\n            few_shot_example_ids=[ex.id for ex in selected],\n        )\n\n        # Log classification with extra metadata\n        if self.logger and publication_id:\n            extra_metadata = None\n            if enforce_precedence and reasoning:\n                # Extract completeness attributes for audit trail (SC-004)\n                extra_metadata = extract_completeness_attributes(reasoning, statement_type)\n                extra_metadata['precedence_applied'] = (original_category != category)\n                extra_metadata['original_category'] = original_category.value if original_category != category else None\n\n            self.logger.log_classification(\n                publication_id=publication_id,\n                classification=classification,\n                statement_text=statement,\n                extra=extra_metadata,\n            )\n\n        return classification\n\n    def classify_publication(\n        self,\n        publication: Publication,\n        return_reasoning: bool = True,\n    ) -> Tuple[Optional[Classification], Optional[Classification]]:\n        \"\"\"Classify both data and code availability for a publication.\n\n        Args:\n            publication: Publication with data/code statements\n            return_reasoning: Include reasoning in results\n\n        Returns:\n            Tuple of (data_classification, code_classification)\n            Either can be None if statement is missing\n        \"\"\"\n        data_result = None\n        code_result = None\n\n        if publication.has_data_statement():\n            data_result = self.classify_statement(\n                statement=publication.data_statement,\n                statement_type=ClassificationType.DATA,\n                return_reasoning=return_reasoning,\n                publication_id=publication.id,\n            )\n\n        if publication.has_code_statement():\n            code_result = self.classify_statement(\n                statement=publication.code_statement,\n                statement_type=ClassificationType.CODE,\n                return_reasoning=return_reasoning,\n                publication_id=publication.id,\n            )\n\n        return data_result, code_result"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convenience Functions\n",
    "\n",
    "Module-level functions for simpler usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "_default_classifier: Optional[OpennessClassifier] = None\n",
    "\n",
    "\n",
    "def get_classifier(config: Optional[ClassifierConfig] = None) -> OpennessClassifier:\n",
    "    \"\"\"Get or create the default classifier instance.\n",
    "    \n",
    "    Args:\n",
    "        config: Optional configuration (loads from env if not provided)\n",
    "        \n",
    "    Returns:\n",
    "        OpennessClassifier instance\n",
    "    \"\"\"\n",
    "    global _default_classifier\n",
    "    \n",
    "    if _default_classifier is None or config is not None:\n",
    "        if config is None:\n",
    "            config = load_config()\n",
    "        _default_classifier = OpennessClassifier.from_config(config)\n",
    "    \n",
    "    return _default_classifier\n",
    "\n",
    "\n",
    "def classify_statement(\n",
    "    statement: str,\n",
    "    statement_type: ClassificationType | str,\n",
    "    config: Optional[ClassifierConfig] = None,\n",
    "    return_reasoning: bool = True,\n",
    ") -> Classification:\n",
    "    \"\"\"Classify a single availability statement.\n",
    "    \n",
    "    Convenience function that manages classifier lifecycle.\n",
    "    \n",
    "    Args:\n",
    "        statement: The availability statement text\n",
    "        statement_type: \"data\" or \"code\" (or ClassificationType)\n",
    "        config: Optional configuration\n",
    "        return_reasoning: Include reasoning in result\n",
    "        \n",
    "    Returns:\n",
    "        Classification result\n",
    "        \n",
    "    Example:\n",
    "        >>> result = classify_statement(\n",
    "        ...     \"Data available at https://zenodo.org/record/12345\",\n",
    "        ...     \"data\"\n",
    "        ... )\n",
    "        >>> print(result.category.value)  # 'open'\n",
    "    \"\"\"\n",
    "    # Convert string to enum if needed\n",
    "    if isinstance(statement_type, str):\n",
    "        statement_type = ClassificationType(statement_type.lower())\n",
    "    \n",
    "    classifier = get_classifier(config)\n",
    "    return classifier.classify_statement(\n",
    "        statement=statement,\n",
    "        statement_type=statement_type,\n",
    "        return_reasoning=return_reasoning,\n",
    "    )\n",
    "\n",
    "\n",
    "def classify_publication(\n",
    "    data_statement: Optional[str] = None,\n",
    "    code_statement: Optional[str] = None,\n",
    "    publication_id: str = \"unknown\",\n",
    "    config: Optional[ClassifierConfig] = None,\n",
    ") -> Tuple[Optional[Classification], Optional[Classification]]:\n",
    "    \"\"\"Classify data and code availability for a publication.\n",
    "    \n",
    "    Args:\n",
    "        data_statement: Data availability statement (optional)\n",
    "        code_statement: Code availability statement (optional)\n",
    "        publication_id: Identifier for logging\n",
    "        config: Optional configuration\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (data_classification, code_classification)\n",
    "    \"\"\"\n",
    "    pub = Publication(\n",
    "        id=publication_id,\n",
    "        data_statement=data_statement,\n",
    "        code_statement=code_statement,\n",
    "    )\n",
    "    \n",
    "    classifier = get_classifier(config)\n",
    "    return classifier.classify_publication(pub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low Confidence Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef identify_low_confidence(\n    classifications: List[Classification],\n    threshold: float = 0.5\n) -> List[Classification]:\n    \"\"\"Identify classifications with low confidence scores.\n\n    Use this to find statements that may need manual review.\n\n    Args:\n        classifications: List of classification results\n        threshold: Confidence threshold (default: 0.5)\n\n    Returns:\n        List of low-confidence classifications\n    \"\"\"\n    return [c for c in classifications if c.confidence_score < threshold]\n\n\ndef suggest_training_examples(\n    classifications: List[Tuple[str, Classification]],\n    threshold: float = 0.5,\n    max_suggestions: int = 10\n) -> List[Tuple[str, Classification]]:\n    \"\"\"Suggest statements that would benefit from manual coding.\n\n    Returns low-confidence classifications that should be manually\n    reviewed and potentially added to training data.\n\n    Args:\n        classifications: List of (statement_text, classification) tuples\n        threshold: Confidence threshold\n        max_suggestions: Maximum suggestions to return\n\n    Returns:\n        List of (statement, classification) tuples needing review\n    \"\"\"\n    low_conf = [\n        (stmt, cls) for stmt, cls in classifications\n        if cls.confidence_score < threshold\n    ]\n\n    # Sort by confidence (lowest first)\n    low_conf.sort(key=lambda x: x[1].confidence_score)\n\n    return low_conf[:max_suggestions]\n\n\ndef validate_classification_precedence(\n    category: OpennessCategory,\n    statement: str,\n    reasoning: str,\n) -> Tuple[OpennessCategory, bool]:\n    \"\"\"Apply FR-004 hard precedence rule for substantial access barriers.\n\n    CRITICAL RULE: If substantial access barriers exist (data use agreements,\n    proprietary terms, confidentiality restrictions), classification MUST be\n    mostly_closed or closed, REGARDLESS of completeness or repository quality.\n\n    This function validates and potentially corrects LLM classifications to\n    ensure the hard precedence rule is always enforced.\n\n    Args:\n        category: The LLM's initial classification\n        statement: The original availability statement\n        reasoning: The LLM's reasoning text\n\n    Returns:\n        Tuple of (corrected_category, precedence_was_applied)\n        - corrected_category: The validated/corrected classification\n        - precedence_was_applied: True if a correction was made\n\n    Examples:\n        >>> # Statement mentions DUA but LLM classified as mostly_open\n        >>> category, applied = validate_classification_precedence(\n        ...     OpennessCategory.MOSTLY_OPEN,\n        ...     \"Data available via data use agreement from ICPSR\",\n        ...     \"High completeness, all data types available...\"\n        ... )\n        >>> category == OpennessCategory.MOSTLY_CLOSED and applied == True\n        True\n\n        >>> # Statement has no barriers, mostly_open classification preserved\n        >>> category, applied = validate_classification_precedence(\n        ...     OpennessCategory.MOSTLY_OPEN,\n        ...     \"All data available on Zenodo with free registration\",\n        ...     \"High completeness with minor registration barrier...\"\n        ... )\n        >>> category == OpennessCategory.MOSTLY_OPEN and applied == False\n        True\n    \"\"\"\n    # Check if substantial barrier exists in statement\n    statement_has_barrier = has_substantial_barrier(statement)\n\n    # \"Upon request\" or \"contact author\" is always CLOSED (not just mostly_closed)\n    statement_lower = statement.lower()\n    is_upon_request = any(phrase in statement_lower for phrase in [\n        'upon request', 'upon reasonable request', 'contact the author',\n        'contact author', 'available from the author', 'request from'\n    ])\n\n    # Determine if correction needed\n    precedence_applied = False\n    corrected_category = category\n\n    if is_upon_request:\n        # \"Upon request\" ALWAYS means CLOSED\n        if category != OpennessCategory.CLOSED:\n            corrected_category = OpennessCategory.CLOSED\n            precedence_applied = True\n    elif statement_has_barrier:\n        # Substantial barrier (but not \"upon request\") -> at most mostly_closed\n        if category in [OpennessCategory.OPEN, OpennessCategory.MOSTLY_OPEN]:\n            corrected_category = OpennessCategory.MOSTLY_CLOSED\n            precedence_applied = True\n\n    return corrected_category, precedence_applied"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test FR-004 precedence validation with boundary cases\nfrom openness_classifier.core import OpennessCategory, ClassificationType\n\nprint(\"=== Testing FR-004 Hard Precedence Rule (T014) ===\\n\")\n\n# Test case 1: DUA statement should be mostly_closed\ncat1, applied1 = validate_classification_precedence(\n    OpennessCategory.MOSTLY_OPEN,\n    \"All data available via data use agreement from ICPSR\",\n    \"High completeness data\"\n)\nprint(f\"Test 1 - DUA present (LLM: mostly_open):\")\nprint(f\"  Result: {cat1.value}, Precedence applied: {applied1}\")\nassert cat1 == OpennessCategory.MOSTLY_CLOSED and applied1 == True\n\n# Test case 2: \"Upon request\" should be closed\ncat2, applied2 = validate_classification_precedence(\n    OpennessCategory.MOSTLY_OPEN,\n    \"Data available upon reasonable request from the corresponding author\",\n    \"Some data\"\n)\nprint(f\"\\nTest 2 - 'Upon request' present (LLM: mostly_open):\")\nprint(f\"  Result: {cat2.value}, Precedence applied: {applied2}\")\nassert cat2 == OpennessCategory.CLOSED and applied2 == True\n\n# Test case 3: No barriers, classification preserved\ncat3, applied3 = validate_classification_precedence(\n    OpennessCategory.MOSTLY_OPEN,\n    \"Raw; Results; Source Data available at Figshare with free registration\",\n    \"High completeness\"\n)\nprint(f\"\\nTest 3 - No substantial barriers (LLM: mostly_open):\")\nprint(f\"  Result: {cat3.value}, Precedence applied: {applied3}\")\nassert cat3 == OpennessCategory.MOSTLY_OPEN and applied3 == False\n\n# Test case 4: GitHub with all code should be mostly_open\ncat4, applied4 = validate_classification_precedence(\n    OpennessCategory.MOSTLY_OPEN,\n    \"All analysis and figure generation code on GitHub at github.com/user/repo\",\n    \"High completeness\"\n)\nprint(f\"\\nTest 4 - All code on GitHub (LLM: mostly_open):\")\nprint(f\"  Result: {cat4.value}, Precedence applied: {applied4}\")\nassert cat4 == OpennessCategory.MOSTLY_OPEN and applied4 == False\n\nprint(\"\\n=== All FR-004 precedence tests passed! ===\")\nprint(\"\\nClassifier module ready with refined taxonomy!\")\nprint(\"To test with LLM, set up your .env file with API keys and run:\")\nprint(\"  from openness_classifier import classify_statement\")\nprint(\"  result = classify_statement('Data available at Zenodo', 'data')\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}