{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Module\n",
    "\n",
    "> Data loading, training examples, and sentence embeddings.\n",
    "\n",
    "This module handles:\n",
    "- Loading training data from articles_reviewed.csv\n",
    "- Creating TrainingExample objects with ground truth labels\n",
    "- Computing sentence embeddings for kNN example selection\n",
    "- Train/test splitting for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Tuple, Dict, Any\n",
    "from datetime import date\n",
    "import logging\n",
    "\n",
    "from sklearn.model_selection import train_test_split as sklearn_split\n",
    "\n",
    "from openness_classifier.core import (\n",
    "    OpennessCategory,\n",
    "    ClassificationType,\n",
    "    DataError\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Classes\n",
    "\n",
    "### Publication\n",
    "\n",
    "Represents a scholarly article with data and code availability statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class Publication:\n",
    "    \"\"\"A scholarly publication with data and code availability statements.\n",
    "    \n",
    "    Attributes:\n",
    "        id: Unique identifier (DOI, PMID, or internal ID)\n",
    "        data_statement: Data availability statement text (None if missing)\n",
    "        code_statement: Code availability statement text (None if missing)\n",
    "        metadata: Additional publication metadata\n",
    "    \"\"\"\n",
    "    id: str\n",
    "    data_statement: Optional[str] = None\n",
    "    code_statement: Optional[str] = None\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    def has_data_statement(self) -> bool:\n",
    "        \"\"\"Check if publication has a valid data statement.\"\"\"\n",
    "        return bool(self.data_statement and self.data_statement.strip() \n",
    "                   and self.data_statement.lower() != 'nothing')\n",
    "    \n",
    "    def has_code_statement(self) -> bool:\n",
    "        \"\"\"Check if publication has a valid code statement.\"\"\"\n",
    "        return bool(self.code_statement and self.code_statement.strip()\n",
    "                   and self.code_statement.lower() != 'nothing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TrainingExample\n",
    "\n",
    "A manually coded example used for few-shot learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class TrainingExample:\n",
    "    \"\"\"A manually coded training example for few-shot learning.\n",
    "    \n",
    "    Attributes:\n",
    "        id: Unique identifier\n",
    "        statement_text: Data or code availability statement\n",
    "        ground_truth: Human-coded openness classification\n",
    "        statement_type: Whether this is a data or code statement\n",
    "        source: Source of the example (e.g., 'articles_reviewed.csv')\n",
    "        embedding: Sentence embedding for kNN selection (computed lazily)\n",
    "    \"\"\"\n",
    "    id: str\n",
    "    statement_text: str\n",
    "    ground_truth: OpennessCategory\n",
    "    statement_type: ClassificationType\n",
    "    source: str = 'articles_reviewed.csv'\n",
    "    embedding: Optional[np.ndarray] = None\n",
    "    \n",
    "    def to_prompt_example(self) -> str:\n",
    "        \"\"\"Format as a few-shot prompt example.\"\"\"\n",
    "        return f\"\"\"Statement: {self.statement_text}\n",
    "Classification: {self.ground_truth.value}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef load_training_data(\n    path: str | Path,\n    filter_dropped: bool = True,\n    filter_climate: bool = False,\n    min_statement_length: int = 10\n) -> Tuple[List[TrainingExample], List[TrainingExample]]:\n    \"\"\"Load training examples from articles_reviewed.csv.\n    \n    Returns separate lists for data and code training examples.\n    \n    Args:\n        path: Path to articles_reviewed.csv\n        filter_dropped: Exclude rows with dropped=1\n        filter_climate: Only include rows with is_climate=1\n        min_statement_length: Minimum statement length to include\n        \n    Returns:\n        Tuple of (data_examples, code_examples)\n        \n    Raises:\n        DataError: If file not found or required columns missing\n    \"\"\"\n    path = Path(path)\n    if not path.exists():\n        raise DataError(f\"Training data file not found: {path}\")\n    \n    # Load CSV with latin-1 encoding to handle non-UTF-8 characters\n    try:\n        df = pd.read_csv(path, encoding='latin-1')\n    except Exception as e:\n        raise DataError(f\"Error reading training data: {e}\")\n    \n    # Validate required columns\n    required_cols = ['data_statement', 'code_statement', 'data_open', 'code_open']\n    missing_cols = [c for c in required_cols if c not in df.columns]\n    if missing_cols:\n        raise DataError(f\"Missing required columns: {missing_cols}\")\n    \n    # Apply filters\n    if filter_dropped and 'dropped' in df.columns:\n        df = df[df['dropped'] != 1]\n    \n    if filter_climate and 'is_climate' in df.columns:\n        df = df[df['is_climate'] == 1]\n    \n    # Extract training examples\n    data_examples = []\n    code_examples = []\n    \n    for idx, row in df.iterrows():\n        pub_id = row.get('doi', str(idx))\n        \n        # Process data statement\n        data_stmt = row.get('data_statement', '')\n        data_label = row.get('data_open', '')\n        \n        if _is_valid_statement(data_stmt, min_statement_length) and _is_valid_label(data_label):\n            try:\n                data_examples.append(TrainingExample(\n                    id=f\"{pub_id}_data\",\n                    statement_text=str(data_stmt).strip(),\n                    ground_truth=OpennessCategory.from_string(data_label),\n                    statement_type=ClassificationType.DATA,\n                ))\n            except ValueError as e:\n                logging.warning(f\"Skipping row {idx} data: {e}\")\n        \n        # Process code statement\n        code_stmt = row.get('code_statement', '')\n        code_label = row.get('code_open', '')\n        \n        if _is_valid_statement(code_stmt, min_statement_length) and _is_valid_label(code_label):\n            try:\n                code_examples.append(TrainingExample(\n                    id=f\"{pub_id}_code\",\n                    statement_text=str(code_stmt).strip(),\n                    ground_truth=OpennessCategory.from_string(code_label),\n                    statement_type=ClassificationType.CODE,\n                ))\n            except ValueError as e:\n                logging.warning(f\"Skipping row {idx} code: {e}\")\n    \n    logging.info(f\"Loaded {len(data_examples)} data examples and {len(code_examples)} code examples\")\n    \n    return data_examples, code_examples\n\n\ndef _is_valid_statement(stmt: Any, min_length: int) -> bool:\n    \"\"\"Check if statement is valid for training.\"\"\"\n    if pd.isna(stmt) or stmt is None:\n        return False\n    stmt_str = str(stmt).strip().lower()\n    if stmt_str == 'nothing' or stmt_str == '' or len(stmt_str) < min_length:\n        return False\n    return True\n\n\ndef _is_valid_label(label: Any) -> bool:\n    \"\"\"Check if label is valid for training.\"\"\"\n    if pd.isna(label) or label is None:\n        return False\n    label_str = str(label).strip().lower()\n    if label_str == '' or label_str == 'nothing':\n        return False\n    return True"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Embeddings\n",
    "\n",
    "Using sentence-transformers for computing embeddings for kNN example selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EmbeddingModel:\n",
    "    \"\"\"Wrapper for sentence-transformers embedding model.\n",
    "    \n",
    "    Computes sentence embeddings for semantic similarity-based\n",
    "    kNN example selection.\n",
    "    \n",
    "    Example:\n",
    "        >>> model = EmbeddingModel('all-MiniLM-L6-v2')\n",
    "        >>> embedding = model.encode(\"Data available at Zenodo\")\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.model_name = model_name\n",
    "    \n",
    "    def encode(self, text: str | List[str]) -> np.ndarray:\n",
    "        \"\"\"Encode text(s) to embedding vector(s).\n",
    "        \n",
    "        Args:\n",
    "            text: Single string or list of strings\n",
    "            \n",
    "        Returns:\n",
    "            Embedding vector(s) as numpy array\n",
    "        \"\"\"\n",
    "        return self.model.encode(text, convert_to_numpy=True)\n",
    "    \n",
    "    def compute_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:\n",
    "        \"\"\"Compute cosine similarity between two embeddings.\"\"\"\n",
    "        return float(np.dot(embedding1, embedding2) / \n",
    "                    (np.linalg.norm(embedding1) * np.linalg.norm(embedding2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_embeddings(\n",
    "    examples: List[TrainingExample],\n",
    "    model: EmbeddingModel\n",
    ") -> List[TrainingExample]:\n",
    "    \"\"\"Compute embeddings for a list of training examples.\n",
    "    \n",
    "    Modifies examples in-place to add embedding field.\n",
    "    \n",
    "    Args:\n",
    "        examples: List of training examples\n",
    "        model: EmbeddingModel to use\n",
    "        \n",
    "    Returns:\n",
    "        Same list with embeddings computed\n",
    "    \"\"\"\n",
    "    if not examples:\n",
    "        return examples\n",
    "    \n",
    "    # Batch encode for efficiency\n",
    "    texts = [ex.statement_text for ex in examples]\n",
    "    embeddings = model.encode(texts)\n",
    "    \n",
    "    # Assign embeddings\n",
    "    for ex, emb in zip(examples, embeddings):\n",
    "        ex.embedding = emb\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_test_split(\n",
    "    examples: List[TrainingExample],\n",
    "    test_size: float = 0.2,\n",
    "    stratify: bool = True,\n",
    "    random_state: int = 42\n",
    ") -> Tuple[List[TrainingExample], List[TrainingExample]]:\n",
    "    \"\"\"Split training examples into train and test sets.\n",
    "    \n",
    "    Args:\n",
    "        examples: List of training examples\n",
    "        test_size: Fraction for test set (default: 0.2)\n",
    "        stratify: Whether to stratify by ground_truth label\n",
    "        random_state: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (train_examples, test_examples)\n",
    "    \"\"\"\n",
    "    if len(examples) < 5:\n",
    "        logging.warning(f\"Very few examples ({len(examples)}), not splitting\")\n",
    "        return examples, []\n",
    "    \n",
    "    labels = [ex.ground_truth.value for ex in examples] if stratify else None\n",
    "    \n",
    "    # Check if stratification is possible (need at least 2 samples per class)\n",
    "    if stratify:\n",
    "        from collections import Counter\n",
    "        label_counts = Counter(labels)\n",
    "        if min(label_counts.values()) < 2:\n",
    "            logging.warning(\"Some classes have <2 samples, disabling stratification\")\n",
    "            labels = None\n",
    "    \n",
    "    train, test = sklearn_split(\n",
    "        examples,\n",
    "        test_size=test_size,\n",
    "        stratify=labels,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    return list(train), list(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def validate_training_data(\n",
    "    examples: List[TrainingExample],\n",
    "    min_class_fraction: float = 0.1\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Validate training data quality and class balance.\n",
    "    \n",
    "    Args:\n",
    "        examples: Training examples to validate\n",
    "        min_class_fraction: Minimum fraction for any class (warn if below)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with validation results and statistics\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    \n",
    "    if not examples:\n",
    "        return {'valid': False, 'error': 'No examples provided'}\n",
    "    \n",
    "    # Class distribution\n",
    "    labels = [ex.ground_truth.value for ex in examples]\n",
    "    distribution = Counter(labels)\n",
    "    total = len(examples)\n",
    "    \n",
    "    # Check class balance\n",
    "    warnings = []\n",
    "    for label, count in distribution.items():\n",
    "        fraction = count / total\n",
    "        if fraction < min_class_fraction:\n",
    "            warnings.append(\n",
    "                f\"Class '{label}' has only {count} examples ({fraction:.1%}), \"\n",
    "                f\"below threshold of {min_class_fraction:.1%}\"\n",
    "            )\n",
    "    \n",
    "    # Statement length statistics\n",
    "    lengths = [len(ex.statement_text) for ex in examples]\n",
    "    \n",
    "    return {\n",
    "        'valid': len(warnings) == 0,\n",
    "        'total_examples': total,\n",
    "        'class_distribution': dict(distribution),\n",
    "        'class_fractions': {k: v/total for k, v in distribution.items()},\n",
    "        'warnings': warnings,\n",
    "        'statement_length': {\n",
    "            'min': min(lengths),\n",
    "            'max': max(lengths),\n",
    "            'mean': np.mean(lengths),\n",
    "            'median': np.median(lengths),\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def reload_training_data(\n",
    "    path: str | Path,\n",
    "    embedding_model: Optional[EmbeddingModel] = None,\n",
    "    **kwargs\n",
    ") -> Tuple[List[TrainingExample], List[TrainingExample]]:\n",
    "    \"\"\"Reload training data and recompute embeddings.\n",
    "    \n",
    "    Use this after updating the training CSV to refresh examples.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to training data CSV\n",
    "        embedding_model: Model for computing embeddings (creates new if None)\n",
    "        **kwargs: Additional arguments passed to load_training_data\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (data_examples, code_examples) with embeddings\n",
    "    \"\"\"\n",
    "    data_examples, code_examples = load_training_data(path, **kwargs)\n",
    "    \n",
    "    if embedding_model is None:\n",
    "        embedding_model = EmbeddingModel()\n",
    "    \n",
    "    compute_embeddings(data_examples, embedding_model)\n",
    "    compute_embeddings(code_examples, embedding_model)\n",
    "    \n",
    "    logging.info(\n",
    "        f\"Reloaded {len(data_examples)} data and {len(code_examples)} code examples \"\n",
    "        f\"with embeddings\"\n",
    "    )\n",
    "    \n",
    "    return data_examples, code_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data loading\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if we're in the right directory\n",
    "test_path = Path('resources/abpoll-open-b71bd12/data/processed/articles_reviewed.csv')\n",
    "if not test_path.exists():\n",
    "    # Try from repo root\n",
    "    test_path = Path('/home/user/open_sesame/resources/abpoll-open-b71bd12/data/processed/articles_reviewed.csv')\n",
    "\n",
    "if test_path.exists():\n",
    "    data_ex, code_ex = load_training_data(test_path)\n",
    "    print(f\"Loaded {len(data_ex)} data examples and {len(code_ex)} code examples\")\n",
    "    \n",
    "    # Show class distribution\n",
    "    if data_ex:\n",
    "        validation = validate_training_data(data_ex)\n",
    "        print(f\"Data class distribution: {validation['class_distribution']}\")\n",
    "        if validation['warnings']:\n",
    "            print(f\"Warnings: {validation['warnings']}\")\n",
    "else:\n",
    "    print(f\"Test file not found at {test_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}