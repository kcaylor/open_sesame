{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Processing Module\n",
    "\n",
    "> Batch classification of multiple publications from CSV files.\n",
    "\n",
    "This module provides:\n",
    "- `classify_csv()`: Process a CSV file with multiple publications\n",
    "- `BatchJob`: Track batch processing progress and statistics\n",
    "- Error handling with skip/fail/log options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from typing import Optional, List, Tuple, Callable, Literal\n",
    "import logging\n",
    "import json\n",
    "\n",
    "from openness_classifier.core import (\n",
    "    OpennessCategory,\n",
    "    ClassificationType,\n",
    "    Classification,\n",
    "    BatchStatus,\n",
    "    ClassificationError,\n",
    "    DataError,\n",
    ")\n",
    "from openness_classifier.config import ClassifierConfig, load_config\n",
    "from openness_classifier.data import Publication\n",
    "from openness_classifier.classifier import OpennessClassifier, get_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchJob\n",
    "\n",
    "Tracks batch processing progress and statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class BatchJob:\n",
    "    \"\"\"Tracks batch processing of multiple publications.\n",
    "    \n",
    "    Attributes:\n",
    "        job_id: Unique identifier for this job\n",
    "        input_file: Path to input CSV\n",
    "        output_file: Path to output CSV\n",
    "        total_publications: Number of publications to process\n",
    "        processed_count: Number processed so far\n",
    "        failed_count: Number that failed\n",
    "        status: Current job status\n",
    "        start_time: When job started\n",
    "        end_time: When job finished (if complete)\n",
    "        error_log: List of (publication_id, error_message) tuples\n",
    "    \"\"\"\n",
    "    job_id: str = field(default_factory=lambda: str(uuid.uuid4())[:8])\n",
    "    input_file: Optional[Path] = None\n",
    "    output_file: Optional[Path] = None\n",
    "    total_publications: int = 0\n",
    "    processed_count: int = 0\n",
    "    failed_count: int = 0\n",
    "    skipped_count: int = 0\n",
    "    status: BatchStatus = BatchStatus.PENDING\n",
    "    start_time: Optional[datetime] = None\n",
    "    end_time: Optional[datetime] = None\n",
    "    error_log: List[Tuple[str, str]] = field(default_factory=list)\n",
    "    \n",
    "    @property\n",
    "    def success_count(self) -> int:\n",
    "        \"\"\"Number of successful classifications.\"\"\"\n",
    "        return self.processed_count - self.failed_count - self.skipped_count\n",
    "    \n",
    "    @property\n",
    "    def progress_percent(self) -> float:\n",
    "        \"\"\"Progress as percentage.\"\"\"\n",
    "        if self.total_publications == 0:\n",
    "            return 0.0\n",
    "        return 100.0 * self.processed_count / self.total_publications\n",
    "    \n",
    "    @property\n",
    "    def duration_seconds(self) -> Optional[float]:\n",
    "        \"\"\"Job duration in seconds.\"\"\"\n",
    "        if not self.start_time:\n",
    "            return None\n",
    "        end = self.end_time or datetime.now()\n",
    "        return (end - self.start_time).total_seconds()\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"Convert to dictionary for serialization.\"\"\"\n",
    "        return {\n",
    "            'job_id': self.job_id,\n",
    "            'input_file': str(self.input_file) if self.input_file else None,\n",
    "            'output_file': str(self.output_file) if self.output_file else None,\n",
    "            'total_publications': self.total_publications,\n",
    "            'processed_count': self.processed_count,\n",
    "            'success_count': self.success_count,\n",
    "            'failed_count': self.failed_count,\n",
    "            'skipped_count': self.skipped_count,\n",
    "            'status': self.status.value,\n",
    "            'start_time': self.start_time.isoformat() if self.start_time else None,\n",
    "            'end_time': self.end_time.isoformat() if self.end_time else None,\n",
    "            'duration_seconds': self.duration_seconds,\n",
    "            'error_count': len(self.error_log),\n",
    "        }\n",
    "    \n",
    "    def summary(self) -> str:\n",
    "        \"\"\"Human-readable summary of job status.\"\"\"\n",
    "        lines = [\n",
    "            f\"BatchJob {self.job_id}\",\n",
    "            f\"Status: {self.status.value}\",\n",
    "            f\"Progress: {self.processed_count}/{self.total_publications} ({self.progress_percent:.1f}%)\",\n",
    "            f\"Success: {self.success_count}, Failed: {self.failed_count}, Skipped: {self.skipped_count}\",\n",
    "        ]\n",
    "        if self.duration_seconds:\n",
    "            lines.append(f\"Duration: {self.duration_seconds:.1f}s\")\n",
    "        return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def classify_csv(\n",
    "    input_path: str | Path,\n",
    "    output_path: Optional[str | Path] = None,\n",
    "    config: Optional[ClassifierConfig] = None,\n",
    "    id_column: str = 'doi',\n",
    "    data_statement_column: str = 'data_statement',\n",
    "    code_statement_column: str = 'code_statement',\n",
    "    error_handling: Literal['skip', 'fail', 'log'] = 'log',\n",
    "    progress_callback: Optional[Callable[[int, int], None]] = None,\n",
    ") -> BatchJob:\n",
    "    \"\"\"Classify publications from a CSV file.\n",
    "    \n",
    "    Reads publications from input CSV, classifies data and code availability,\n",
    "    and writes results to output CSV.\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to input CSV file\n",
    "        output_path: Path for output CSV (default: input_classified.csv)\n",
    "        config: Classifier configuration\n",
    "        id_column: Column name for publication ID\n",
    "        data_statement_column: Column name for data statements\n",
    "        code_statement_column: Column name for code statements\n",
    "        error_handling: How to handle errors:\n",
    "            - 'skip': Skip failed rows, continue processing\n",
    "            - 'fail': Stop on first error\n",
    "            - 'log': Log error and continue (default)\n",
    "        progress_callback: Optional callback(processed, total) for progress\n",
    "        \n",
    "    Returns:\n",
    "        BatchJob with statistics and any errors\n",
    "    \"\"\"\n",
    "    input_path = Path(input_path)\n",
    "    if output_path is None:\n",
    "        output_path = input_path.with_suffix('.classified.csv')\n",
    "    else:\n",
    "        output_path = Path(output_path)\n",
    "    \n",
    "    # Initialize job\n",
    "    job = BatchJob(\n",
    "        input_file=input_path,\n",
    "        output_file=output_path,\n",
    "        status=BatchStatus.PENDING,\n",
    "    )\n",
    "    \n",
    "    # Load input CSV\n",
    "    try:\n",
    "        df = pd.read_csv(input_path)\n",
    "    except Exception as e:\n",
    "        job.status = BatchStatus.FAILED\n",
    "        job.error_log.append(('_load', str(e)))\n",
    "        raise DataError(f\"Failed to read input CSV: {e}\")\n",
    "    \n",
    "    job.total_publications = len(df)\n",
    "    job.status = BatchStatus.RUNNING\n",
    "    job.start_time = datetime.now()\n",
    "    \n",
    "    # Get classifier\n",
    "    classifier = get_classifier(config)\n",
    "    \n",
    "    # Add output columns\n",
    "    df['data_classification'] = None\n",
    "    df['data_confidence'] = None\n",
    "    df['code_classification'] = None\n",
    "    df['code_confidence'] = None\n",
    "    \n",
    "    # Process each row\n",
    "    for idx, row in df.iterrows():\n",
    "        pub_id = str(row.get(id_column, idx))\n",
    "        \n",
    "        try:\n",
    "            # Create publication\n",
    "            pub = Publication(\n",
    "                id=pub_id,\n",
    "                data_statement=_get_statement(row, data_statement_column),\n",
    "                code_statement=_get_statement(row, code_statement_column),\n",
    "            )\n",
    "            \n",
    "            # Classify\n",
    "            data_cls, code_cls = classifier.classify_publication(pub)\n",
    "            \n",
    "            # Store results\n",
    "            if data_cls:\n",
    "                df.at[idx, 'data_classification'] = data_cls.category.value\n",
    "                df.at[idx, 'data_confidence'] = data_cls.confidence_score\n",
    "            \n",
    "            if code_cls:\n",
    "                df.at[idx, 'code_classification'] = code_cls.category.value\n",
    "                df.at[idx, 'code_confidence'] = code_cls.confidence_score\n",
    "            \n",
    "            # Track skipped (no statements)\n",
    "            if not data_cls and not code_cls:\n",
    "                job.skipped_count += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            job.failed_count += 1\n",
    "            job.error_log.append((pub_id, str(e)))\n",
    "            \n",
    "            if error_handling == 'fail':\n",
    "                job.status = BatchStatus.FAILED\n",
    "                job.end_time = datetime.now()\n",
    "                raise\n",
    "            elif error_handling == 'log':\n",
    "                logging.error(f\"Failed to classify {pub_id}: {e}\")\n",
    "            # 'skip' just continues\n",
    "        \n",
    "        job.processed_count += 1\n",
    "        \n",
    "        if progress_callback:\n",
    "            progress_callback(job.processed_count, job.total_publications)\n",
    "    \n",
    "    # Save output\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    job.status = BatchStatus.COMPLETED\n",
    "    job.end_time = datetime.now()\n",
    "    \n",
    "    # Log batch job details\n",
    "    _log_batch_job(job, config)\n",
    "    \n",
    "    return job\n",
    "\n",
    "\n",
    "def _get_statement(row: pd.Series, column: str) -> Optional[str]:\n",
    "    \"\"\"Extract statement from row, handling missing values.\"\"\"\n",
    "    if column not in row.index:\n",
    "        return None\n",
    "    value = row[column]\n",
    "    if pd.isna(value) or str(value).strip().lower() in ('', 'nothing', 'nan'):\n",
    "        return None\n",
    "    return str(value).strip()\n",
    "\n",
    "\n",
    "def _log_batch_job(job: BatchJob, config: Optional[ClassifierConfig]) -> None:\n",
    "    \"\"\"Log batch job details to log directory.\"\"\"\n",
    "    if config:\n",
    "        log_path = config.log_dir / f\"batch_{job.job_id}.json\"\n",
    "    else:\n",
    "        log_path = Path('logs') / f\"batch_{job.job_id}.json\"\n",
    "    \n",
    "    log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(log_path, 'w') as f:\n",
    "        json.dump(job.to_dict(), f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test BatchJob\n",
    "job = BatchJob(\n",
    "    total_publications=100,\n",
    "    processed_count=50,\n",
    "    failed_count=2,\n",
    "    skipped_count=5,\n",
    "    status=BatchStatus.RUNNING\n",
    ")\n",
    "\n",
    "print(f\"Success count: {job.success_count}\")\n",
    "print(f\"Progress: {job.progress_percent}%\")\n",
    "print(job.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
