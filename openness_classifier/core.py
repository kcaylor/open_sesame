# AUTOGENERATED FROM nbs/00_core.ipynb - DO NOT EDIT
"""Core module with base types, enumerations, and error classes."""

from __future__ import annotations
from enum import Enum
from dataclasses import dataclass, field
from datetime import datetime
from typing import Optional, Dict, Any, List
from pathlib import Path
import json
import hashlib
import time
import logging
import os


class OpennessCategory(str, Enum):
    """4-category ordinal taxonomy for data/code openness classification.

    Categories are ordered from most open to least open:
    open > mostly_open > mostly_closed > closed
    """
    OPEN = "open"
    MOSTLY_OPEN = "mostly_open"
    MOSTLY_CLOSED = "mostly_closed"
    CLOSED = "closed"

    @classmethod
    def from_string(cls, value: str) -> 'OpennessCategory':
        """Parse category from string, handling various formats."""
        normalized = value.lower().strip().replace(' ', '_').replace('-', '_')

        mapping = {
            'closed': cls.CLOSED,
            'partially_closed': cls.MOSTLY_CLOSED,
            'mostly_closed': cls.MOSTLY_CLOSED,
            'partially_open': cls.MOSTLY_OPEN,
            'mostly_open': cls.MOSTLY_OPEN,
            'open': cls.OPEN,
        }

        if normalized in mapping:
            return mapping[normalized]

        raise ValueError(f"Unknown openness category: {value}")

    def __lt__(self, other: 'OpennessCategory') -> bool:
        order = [self.CLOSED, self.MOSTLY_CLOSED, self.MOSTLY_OPEN, self.OPEN]
        return order.index(self) < order.index(other)

    def __le__(self, other: 'OpennessCategory') -> bool:
        return self == other or self < other


class ClassificationType(str, Enum):
    """Type of availability statement being classified."""
    DATA = "data"
    CODE = "code"


class LLMProviderType(str, Enum):
    """Supported LLM provider types."""
    CLAUDE = "claude"
    OPENAI = "openai"
    OLLAMA = "ollama"


class BatchStatus(str, Enum):
    """Status of a batch processing job."""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"


# Error Classes
class ClassificationError(Exception):
    """Base exception for classification errors."""
    pass


class LLMError(ClassificationError):
    """Error from LLM provider (API failure, rate limit, etc.)."""
    def __init__(self, message: str, provider: str = None, retryable: bool = False):
        super().__init__(message)
        self.provider = provider
        self.retryable = retryable


class ConfigurationError(ClassificationError):
    """Error in configuration (missing API key, invalid settings)."""
    pass


class DataError(ClassificationError):
    """Error in data loading or processing."""
    pass


class ValidationError(ClassificationError):
    """Error in validation (invalid category, missing ground truth)."""
    pass


@dataclass
class LLMConfiguration:
    """Configuration for LLM provider, tracked for reproducibility."""
    provider: LLMProviderType
    model_name: str
    temperature: float = 0.1
    max_tokens: int = 500
    top_p: float = 0.95
    api_endpoint: Optional[str] = None
    api_key_hash: Optional[str] = None
    configuration_timestamp: datetime = field(default_factory=datetime.utcnow)

    def to_dict(self) -> Dict[str, Any]:
        return {
            'provider': self.provider.value,
            'model_name': self.model_name,
            'temperature': self.temperature,
            'max_tokens': self.max_tokens,
            'top_p': self.top_p,
            'api_endpoint': self.api_endpoint,
            'api_key_hash': self.api_key_hash,
            'configuration_timestamp': self.configuration_timestamp.isoformat(),
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'LLMConfiguration':
        return cls(
            provider=LLMProviderType(data['provider']),
            model_name=data['model_name'],
            temperature=data.get('temperature', 0.1),
            max_tokens=data.get('max_tokens', 500),
            top_p=data.get('top_p', 0.95),
            api_endpoint=data.get('api_endpoint'),
            api_key_hash=data.get('api_key_hash'),
            configuration_timestamp=datetime.fromisoformat(data['configuration_timestamp'])
                if 'configuration_timestamp' in data else datetime.utcnow(),
        )

    def to_json(self) -> str:
        return json.dumps(self.to_dict(), indent=2)

    @staticmethod
    def hash_api_key(api_key: str) -> str:
        return hashlib.sha256(api_key.encode()).hexdigest()[:16]


@dataclass
class Classification:
    """Result of classifying a data or code availability statement."""
    category: OpennessCategory
    statement_type: ClassificationType
    confidence_score: float = 0.8
    reasoning: Optional[str] = None
    timestamp: datetime = field(default_factory=datetime.utcnow)
    model_config: Optional[LLMConfiguration] = None
    few_shot_example_ids: List[str] = field(default_factory=list)

    def __post_init__(self):
        if not 0 <= self.confidence_score <= 1:
            raise ValueError(f"confidence_score must be between 0 and 1, got {self.confidence_score}")

    def to_dict(self) -> Dict[str, Any]:
        return {
            'category': self.category.value,
            'statement_type': self.statement_type.value,
            'confidence_score': self.confidence_score,
            'reasoning': self.reasoning,
            'timestamp': self.timestamp.isoformat(),
            'model_config': self.model_config.to_dict() if self.model_config else None,
            'few_shot_example_ids': self.few_shot_example_ids,
        }


class LLMProvider:
    """Unified LLM provider interface using LiteLLM."""

    def __init__(self, config: LLMConfiguration):
        self.config = config
        self._setup_provider()

    def _setup_provider(self):
        if self.config.provider == LLMProviderType.CLAUDE:
            self.model_id = self.config.model_name
            if not self.model_id.startswith('claude'):
                self.model_id = f"claude/{self.model_id}"
        elif self.config.provider == LLMProviderType.OPENAI:
            self.model_id = self.config.model_name
        elif self.config.provider == LLMProviderType.OLLAMA:
            self.model_id = f"ollama/{self.config.model_name}"
            if self.config.api_endpoint:
                os.environ['OLLAMA_API_BASE'] = self.config.api_endpoint
        else:
            raise ConfigurationError(f"Unknown provider: {self.config.provider}")

    def complete(self, prompt: str, max_retries: int = 3, retry_delay: float = 1.0) -> str:
        import litellm

        last_error = None
        delay = retry_delay

        for attempt in range(max_retries + 1):
            try:
                response = litellm.completion(
                    model=self.model_id,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=self.config.temperature,
                    max_tokens=self.config.max_tokens,
                    top_p=self.config.top_p,
                )
                return response.choices[0].message.content
            except Exception as e:
                last_error = e
                error_str = str(e).lower()

                retryable = any(x in error_str for x in [
                    'rate limit', 'timeout', 'overloaded',
                    'service unavailable', '529', '503', '504'
                ])

                if retryable and attempt < max_retries:
                    logging.warning(
                        f"LLM request failed (attempt {attempt + 1}/{max_retries + 1}): {e}. "
                        f"Retrying in {delay:.1f}s..."
                    )
                    time.sleep(delay)
                    delay *= 2
                else:
                    break

        raise LLMError(
            f"LLM request failed after {max_retries + 1} attempts: {last_error}",
            provider=self.config.provider.value,
            retryable=False
        )


class ClassificationLogger:
    """Logger for classification decisions in JSON Lines format."""

    def __init__(self, log_path: str | Path):
        self.log_path = Path(log_path)
        self.log_path.parent.mkdir(parents=True, exist_ok=True)

    def log_classification(
        self,
        publication_id: str,
        classification: Classification,
        statement_text: str,
        extra: Optional[Dict[str, Any]] = None
    ):
        log_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'publication_id': publication_id,
            'statement_type': classification.statement_type.value,
            'statement_text': statement_text[:500],
            'classification': classification.to_dict(),
        }

        if extra:
            log_entry['extra'] = extra

        with open(self.log_path, 'a') as f:
            f.write(json.dumps(log_entry) + '\n')

    def log_error(
        self,
        publication_id: str,
        error: Exception,
        context: Optional[Dict[str, Any]] = None
    ):
        log_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'publication_id': publication_id,
            'error': True,
            'error_type': type(error).__name__,
            'error_message': str(error),
        }

        if context:
            log_entry['context'] = context

        with open(self.log_path, 'a') as f:
            f.write(json.dumps(log_entry) + '\n')
