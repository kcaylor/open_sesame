# AUTOGENERATED FROM nbs/02_data.ipynb - DO NOT EDIT
"""Data loading, training examples, and sentence embeddings."""

from __future__ import annotations
import pandas as pd
import numpy as np
from pathlib import Path
from dataclasses import dataclass, field
from typing import Optional, List, Tuple, Dict, Any
from datetime import date
import logging

from sklearn.model_selection import train_test_split as sklearn_split

from openness_classifier.core import (
    OpennessCategory,
    ClassificationType,
    DataError
)


@dataclass
class Publication:
    """A scholarly publication with data and code availability statements."""
    id: str
    data_statement: Optional[str] = None
    code_statement: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

    def has_data_statement(self) -> bool:
        return bool(self.data_statement and self.data_statement.strip()
                   and self.data_statement.lower() != 'nothing')

    def has_code_statement(self) -> bool:
        return bool(self.code_statement and self.code_statement.strip()
                   and self.code_statement.lower() != 'nothing')


@dataclass
class TrainingExample:
    """A manually coded training example for few-shot learning."""
    id: str
    statement_text: str
    ground_truth: OpennessCategory
    statement_type: ClassificationType
    source: str = 'articles_reviewed.csv'
    embedding: Optional[np.ndarray] = None

    def to_prompt_example(self) -> str:
        return f"""Statement: {self.statement_text}
Classification: {self.ground_truth.value}"""


def load_training_data(
    path: str | Path,
    filter_dropped: bool = True,
    filter_climate: bool = False,
    min_statement_length: int = 10
) -> Tuple[List[TrainingExample], List[TrainingExample]]:
    """Load training examples from articles_reviewed.csv."""
    path = Path(path)
    if not path.exists():
        raise DataError(f"Training data file not found: {path}")

    try:
        df = pd.read_csv(path)
    except Exception as e:
        raise DataError(f"Error reading training data: {e}")

    required_cols = ['data_statement', 'code_statement', 'data_open', 'code_open']
    missing_cols = [c for c in required_cols if c not in df.columns]
    if missing_cols:
        raise DataError(f"Missing required columns: {missing_cols}")

    if filter_dropped and 'dropped' in df.columns:
        df = df[df['dropped'] != 1]

    if filter_climate and 'is_climate' in df.columns:
        df = df[df['is_climate'] == 1]

    data_examples = []
    code_examples = []

    for idx, row in df.iterrows():
        pub_id = row.get('doi', str(idx))

        data_stmt = row.get('data_statement', '')
        data_label = row.get('data_open', '')

        if _is_valid_statement(data_stmt, min_statement_length) and _is_valid_label(data_label):
            try:
                data_examples.append(TrainingExample(
                    id=f"{pub_id}_data",
                    statement_text=str(data_stmt).strip(),
                    ground_truth=OpennessCategory.from_string(data_label),
                    statement_type=ClassificationType.DATA,
                ))
            except ValueError as e:
                logging.warning(f"Skipping row {idx} data: {e}")

        code_stmt = row.get('code_statement', '')
        code_label = row.get('code_open', '')

        if _is_valid_statement(code_stmt, min_statement_length) and _is_valid_label(code_label):
            try:
                code_examples.append(TrainingExample(
                    id=f"{pub_id}_code",
                    statement_text=str(code_stmt).strip(),
                    ground_truth=OpennessCategory.from_string(code_label),
                    statement_type=ClassificationType.CODE,
                ))
            except ValueError as e:
                logging.warning(f"Skipping row {idx} code: {e}")

    logging.info(f"Loaded {len(data_examples)} data examples and {len(code_examples)} code examples")

    return data_examples, code_examples


def _is_valid_statement(stmt: Any, min_length: int) -> bool:
    if pd.isna(stmt) or stmt is None:
        return False
    stmt_str = str(stmt).strip().lower()
    if stmt_str == 'nothing' or stmt_str == '' or len(stmt_str) < min_length:
        return False
    return True


def _is_valid_label(label: Any) -> bool:
    if pd.isna(label) or label is None:
        return False
    label_str = str(label).strip().lower()
    if label_str == '' or label_str == 'nothing':
        return False
    return True


class EmbeddingModel:
    """Wrapper for sentence-transformers embedding model."""

    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):
        from sentence_transformers import SentenceTransformer
        self.model = SentenceTransformer(model_name)
        self.model_name = model_name

    def encode(self, text: str | List[str]) -> np.ndarray:
        return self.model.encode(text, convert_to_numpy=True)

    def compute_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        return float(np.dot(embedding1, embedding2) /
                    (np.linalg.norm(embedding1) * np.linalg.norm(embedding2)))


def compute_embeddings(
    examples: List[TrainingExample],
    model: EmbeddingModel
) -> List[TrainingExample]:
    """Compute embeddings for a list of training examples."""
    if not examples:
        return examples

    texts = [ex.statement_text for ex in examples]
    embeddings = model.encode(texts)

    for ex, emb in zip(examples, embeddings):
        ex.embedding = emb

    return examples


def train_test_split(
    examples: List[TrainingExample],
    test_size: float = 0.2,
    stratify: bool = True,
    random_state: int = 42
) -> Tuple[List[TrainingExample], List[TrainingExample]]:
    """Split training examples into train and test sets."""
    if len(examples) < 5:
        logging.warning(f"Very few examples ({len(examples)}), not splitting")
        return examples, []

    labels = [ex.ground_truth.value for ex in examples] if stratify else None

    if stratify:
        from collections import Counter
        label_counts = Counter(labels)
        if min(label_counts.values()) < 2:
            logging.warning("Some classes have <2 samples, disabling stratification")
            labels = None

    train, test = sklearn_split(
        examples,
        test_size=test_size,
        stratify=labels,
        random_state=random_state
    )

    return list(train), list(test)


def validate_training_data(
    examples: List[TrainingExample],
    min_class_fraction: float = 0.1
) -> Dict[str, Any]:
    """Validate training data quality and class balance."""
    from collections import Counter

    if not examples:
        return {'valid': False, 'error': 'No examples provided'}

    labels = [ex.ground_truth.value for ex in examples]
    distribution = Counter(labels)
    total = len(examples)

    warnings = []
    for label, count in distribution.items():
        fraction = count / total
        if fraction < min_class_fraction:
            warnings.append(
                f"Class '{label}' has only {count} examples ({fraction:.1%}), "
                f"below threshold of {min_class_fraction:.1%}"
            )

    lengths = [len(ex.statement_text) for ex in examples]

    return {
        'valid': len(warnings) == 0,
        'total_examples': total,
        'class_distribution': dict(distribution),
        'class_fractions': {k: v/total for k, v in distribution.items()},
        'warnings': warnings,
        'statement_length': {
            'min': min(lengths),
            'max': max(lengths),
            'mean': np.mean(lengths),
            'median': np.median(lengths),
        }
    }


def reload_training_data(
    path: str | Path,
    embedding_model: Optional[EmbeddingModel] = None,
    **kwargs
) -> Tuple[List[TrainingExample], List[TrainingExample]]:
    """Reload training data and recompute embeddings."""
    data_examples, code_examples = load_training_data(path, **kwargs)

    if embedding_model is None:
        embedding_model = EmbeddingModel()

    compute_embeddings(data_examples, embedding_model)
    compute_embeddings(code_examples, embedding_model)

    logging.info(
        f"Reloaded {len(data_examples)} data and {len(code_examples)} code examples "
        f"with embeddings"
    )

    return data_examples, code_examples
